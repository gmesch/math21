<!doctype html>
<html>
  <head>
    <title>Math for ML</title>
    
    <script src="lib/jslib.js"></script>
    
    <script src="lib/slides.js"></script>
    <link href="lib/slides.css" rel="stylesheet">
    
    <script src="lib/state.js"></script>
    <script src="lib/state_util.js"></script>

    <script src="slides_mathjax.js"></script>
    
    <script src="slides_loader.js"></script>
    <script src="prettify/run_prettify.js?autorun=false&lang=scm" async></script>

    <script src="slides_init.js"></script>
    <link href="slides_style.css" rel="stylesheet">

    <script src="https://www.gstatic.com/firebasejs/8.3.1/firebase-app.js"></script>
    <script src="https://www.gstatic.com/firebasejs/8.3.1/firebase-analytics.js"></script>
    <script src="https://www.gstatic.com/firebasejs/8.3.1/firebase-messaging.js"></script>
    <script src="slides_firebase.js"></script>

  </head>

  <body onload="init()">
    
    <div id="bgpage" class="background"></div>
    
    <div class="header">
    </div>

    <div id="footer" class="footer">
      <div id="navigator" class="large"></div>
    </div>

    <div id="slides" class="slides">
      <div class="slide title"
           onshow="d0('navigator','footer');v0('bgpage')"
           onhide="d1('navigator','footer');v1('bgpage')">

        <h1>Linear Algebra for Machine Learning</h1>
        <h2>Tai-Danae, Mesch</h2>
        
        <div class="bottom">
          <h2 class="venue">X/Google, 2021</h2>
        </div>
      </div>
      
      <div class="slide build-focus-visible" group="intro" name="intro">
        <h1>Intro: Tensors!</h1>
        <div>The ML library we all use is called
          Tensorflow, after those things from Linear Algebra. But
          what <em>are</em> Tensors really?</div>
        <div>Important, is what they are!</div>
        <ul>
          <li step="1">In <b>General Relativity</b>, tensors describe
            Gravitation.
            
          <li step="2">In <b>Quantum Mechanics</b>, multi-particle states are
            the tensor products of single particle states.

          <li step="3">In <b>Quantum Computing</b>, each qubit is one factor of
            a tensor product.

          <li step="4">In <b>Machine Learning</b> ... well, those "tensors" in
            Tensorflow are just superficially connected with what they are in
            Mathematics -- we'll explain how exactly! -- but tensor products
            appear in the interesting places when optimizing weight matrices in
            large models.
        </ul>

        <div class="notes">Remember to look at the notes.</div>
      </div>

      <div class="slide build-focus-visible" group="intro" name="plan">
        <h1>Game Plan</h1>
        <div>Throughout the next 3 sessions:</div>

        <ul>
          <li step="1" link="theme1">We explain what tensors are, and how they
            come to be associated with multidimensional array data structures in
            software engineering, especially in machine learning.
            
          <li step="2" link="theme2">We contemplate how tensor products of
            vector spaces have a peculiar scaling behavior in their dimension:
            Unlike the Cartesian Product, which adds the dimensions of its
            factors, the Tensor Product multiplies. We also contemplate how
            confusing it is that $2+2=2*2=2^2$.

          <li step="3" link="theme3">We finally look how tensors and their
            products appear in machine learning models.
        </ul>
      </div>

      <div class="slide build-focus-visible" group="intro">
        <h1>Meta</h1>

        <div>Besides the improved understanding of ML and QC, there are a few
          tangential motivations that are inspiring in their own right, or for
          building software:</div>

        <ul>
          <li step="1">Introduce axiomatic construction.

          <li step="2">Look at Notation.

          <li step="3">Learn the same thing againi from a different
            perspective. (Remember the Tensorflow code retreat.)

          <li step="4">Introduce some mathematical concepts, such as
            Isomorphism and Duality.
        </ul>
      </div>

      <div class="slide noprint" group="theme1" name="theme1">
        <div class="banner">
          <div class="content">
            <b>I - Vectors and Tensors</b><br><em>Why, in software, arrays with multiple
            integer indices came to be called tensors.</em>
          </div>
        </div>
      </div>
      
      <div class="slide build-visible" group="theme1">
        <h1>Vectors and Tensors - Overview</h1>
        <div>Plan for today.</div>
        <ol>
          <li step="0">Recap Axioms of Vector Space

          <li step="1">Einstein Notation

          <li step="2">Linear Independence <em>gives rise to Basis and Coordinates</em>

          <li step="3">Basis and Coordinate Transformations

          <li step="4">The Scalar Product <span step="5">$\leftarrow$ This is
              the first Tensor!</span>

          <li step="6">Tensor Coordinates and Basis Transformations
        </ol>
      </div>

      <div class="slide build-focus-visible" group="theme1-vs">
        <h1>Axioms of Vector Space</h1>
        <div>Recap what Vectors are, and Vector Spaces.</div>

        <ul>
          <li step="0"><b>Vectors</b> can be <b>added</b> to each other and
            <b>multiplied with scalar</b> numbers.
            
          <li step="1"><em>Scalar numbers</em> are elements of a <b>Field</b>,
            usually <b>Rational Numbers</b>, <b>Real Numbers</b>,
            or <b>Complex Numbers</b>, but also <em>finite fields</em>,
            notably the <b>Galois Field</b>.
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-vs">
        <h1>Axioms of Vector Space</h1>
        <div>Every set, whose elements can be added and multiplied with elements
          from a field, according to the following axioms, is a vector space:</div>

        <ol>
          <li step="1">Associative: $(\xvec{a} + \xvec{b}) + \xvec{c} = \xvec{a} + (\xvec{b}
            + \xvec{c})$
          <li step="2">Commutative: $\xvec{a} + \xvec{b} = \xvec{b} + \xvec{a}$</li>
          <li step="3">Neutral Element: $\exists \xvec{0} \; \forall \xvec{a} \,:\, \xvec{a} + \xvec{0} = \xvec{a}$
          <li step="4">Inverse Element: $\forall \xvec{a} \; \exists -\!\xvec{a} \,:\, \xvec{a} + (-\xvec{a}) = \xvec{0}$
          <li step="5">Scalar mult compatible with Field mult: $a (b \xvec{a}) = (a b) \xvec{a}$
          <li step="6">Neutral Element of scalar mult: $1 \xvec{a} = \xvec{a}$
          <li step="7">Distributive over vector add: $a \xvec{a} + a \xvec{b} = a (\xvec{a} + \xvec{b})$
          <li step="8">Distributive over scalar add: $a \xvec{a} + b \xvec{a} = (a + b) \xvec{a}$
        </ol>

        <div class="notes">
          <ul>
            <li>$\forall$ is implied where absent in the axioms.
          </ul>
        </div>
      </div>

      <div class="slide build-focus-visible" group="theme1-vs">
        <h1>Axioms of Vector Space - Examples</h1>
        <div>Two examples of what vector spaces are.</div>
        <ul>
          <li step="1">Example 1: Arrows over Rational Numbers.
            
          <li step="2">Example 2: Tuples of Real Numbers over Real Numbers.
        </ul>
      </div>

      <div class="slide build-focus" group="theme1-vs">
        <h1>Axioms of Vector Space - Example: Arrows</h1>
        <div>Arrows in the 2D plane are vectors, with their operations defined
          by Euclidean Geometry.</div>
        <div class="img">
          <img class="img" src="img/vectors/vector-plus.png" step="1">
          <img class="img" src="img/vectors/vector-plus-result.png" step="2">
          <img class="img" src="img/vectors/vector-neg.png" step="3">
          <img class="img" src="img/vectors/vector-neg-result.png" step="4">
          <img class="img" src="img/vectors/vector-mult.png" step="5">
          <img class="img" src="img/vectors/vector-mult-result.png" step="6">
          <img class="img" src="img/vectors/vector-frac.png" step="7">
          <img class="img" src="img/vectors/vector-frac-result.png" step="8">
        </div>
      </div>
      
      <div class="slide build-focus-visible" group="theme1-vs">
        <h1>Axioms of Vector Space</h1>
        <div><b>Exercise:</b> Are these two examples vector spaces?</div>

        <ul>
          <li>(a) Tuples of Real Numbers over Rational Numbers?

          <li>(b) Tuples of Rational Numbers over Real Numbers?
        </ul>
      </div>
      
      <div class="slide build-focus-visible" group="theme1-li">
        <h1>Linear Independence</h1>
        <div step="0">The existence of <b>coordinates</b> follows from the
          axioms.</div>

        <div class="notes">
          <ul>
            <li>This derivation leads to the introduction of coordinates.

            <li>It follows directly form the algebraic structure introduced by
              the axioms.
          </ul>
        </div>

        <ul>
          <li step="1">Vectors can be <b>linearly combined</b>, i.e. added
            up with weights $w_i$:

            $$\sum_{i} w_i \xvec{v}_i$$

          <li step="2">Can such combinations yield $\xvec{0}$ with coefficients
            that are not all $0$? (It's always possible with all coefficients 0,
            of course.)
            
          <li step="3">Vectors in the combination are then said to
            be <b>linearly dependent</b>.

          <li step="3">Otherwise they are <b>linearly indedendent</b>.
        </ul>

        <div class="notes">
          <ul>
            <li>The vectors $\xvec{v}_i$ in the sum are really <b>multiple
              vectors</b> (one for each index), <b>not</b> the <b>coordinates
              of one vector</b> &mdash; we get to coordinates later!
          </ul>
        </div>
      </div>

      <div class="slide build-focus-visible" group="theme1-li">
        <h1>Linear Independence</h1>
        
        <div><b>Exercise:</b> Imagine sets of arrows that are</div>
        <ul>
          <li step="1">(a) linearly dependent,
          <li step="2">(b) linearly independent.
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-li">
        <h1>Einstein Notation</h1>
        <div>We introduce a notation for vector algebra that's very neat.</div>
        <ul>
          <li>Convention:

            $$w^i \xvec{v}_i := \sum_{i} w_i \xvec{v}_i$$

          <li>If an index $i$ appears in a multiplication expression both as
            upper index $w^i$ and as lower index $\xvec{v}_i$, then the sum over
            the range of the index is implied.

          <li>Also for multiple indices. (Will see this for tensors.)
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-li">
        <h1>Einstein Notation</h1>
        <ul>
          <li>No sum for an index on its own:
            
            $$\xvec{v}_i$$
            
            is simply a tuple of vectors.
          
            $$w^i$$
            
            is simply a tuple of weights, i.e. numbers.
          
          <li>No sum for a repeated index on top or bottom:
            
            $$M_{ii}$$
            
            is the tuple of the diagonal elements of the matrix $M_{ij}$.
          </li>
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-li">
        <h1>Linear Independence</h1>
        <div class="notes">
          More analysis yields interesting properties of linear independence.
        </div>

        <ul>
          <li step="0">In each vector space, the maximum number of elements $\xvec{e}_i$
            in a set of linearly independent vectors is <b>fixed</b>.

          <li step="1,3">For every other vector $\xvec{v}$ added to such a set, there is a
            linear combination that yields $\xvec{0}$:

            $$v^{i}\xvec{e}_{i} + v^{0}\xvec{v} = \xvec{0}$$</li>

          <li step="2,3">Another way of saying this is that $\xvec{v}$ can be combined from
            $\xvec{e}_{i}$. With the same coefficients as above, and noticing that
            $v^{0}$ cannot be $0$:
            
            $$\xvec{v} = - \frac{v^{i}}{v^0} \xvec{e}_{i}$$
            
            (Einstein notation applies.)

          <li step="3"><b>Exercise:</b> (a) why must be $v^{0} \neq 0$? (b) Why
            would it be a problem otherwise?</li>
        </ul>
      </div>


      <div class="slide build-visible" group="theme1-li">
        <h1>Linear Independence</h1>
        <div class="notes">
          One more property, and it all yields 3 important concepts.
        </div>

        <ul>
          <li step="0">Write more simply;
            
            $$\xvec{v} = v^{i} \xvec{e}_{i}$$

          <li step="1">Are these coefficients unique?

          <li step="2">It turns out yes they are!

          <li step="3">Proof follows, to show how such things work ...
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-li">
        <h1>Linear Independence</h1>
        <div step="0"><b>Proof</b> that the coefficients of any vector are unambiguous.</div>

        <ul>
          <li step="1">Assume there are two such sets of coefficients:

            $$\xvec{v} = v^{i} \xvec{e}_{i} = u^{i} \xvec{e}_{i}$$

          <li step="2">subtract one from the other:

            $$\xvec{v} - \xvec{v} = v^{i} \xvec{e}_{i} - u^{i} \xvec{e}_{i}$$

            $$\xvec{0} = (v^{i} - u^{i}) \xvec{e}_{i}$$

          <li step="3">Now remember that $\xvec{e}_i$ are linearly independent and that
            means $\xvec{0}$ can be combined <em>only</em> with <b>all</b>
            coefficients $0$.
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-li">
        <h1>Linear Independence</h1>

        <ul>
          <li step="0">Thus for all $i$:

            $$v^{i} - u^{i} = 0$$

            (No Einstein sum here, because the index is up both times.)

          <li step="1">Or:

            $$v^{i} = u^{i}$$

            QED.
            
          <li step="2">Thus, every vector in the vector space can be represented as a
            combination with unambiguous coefficients from any maximal set of
            lineary independent vectors.</li>
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-bt">
        <h1>Dimension, Basis, and Coordinates</h1>
        <div step="0">From the concept of Linear Indepdendece, we arrive
          at <b>3 Definitions</b>:</div>

        <ul>
          <li step="1">The maximum number of elements in a set of linearly
            independent vectors of a vector space is the <b>Dimension</b> of
            that vector space.
            
          <li step="2">Any such set itself is a <b>Basis</b> of that vector
            space.
            
          <li step="3">The coefficients in the linear combination of the basis
            vectors that yields a vector are the <b>Coordinates</b> of the
            vector in that basis.
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-bt">
        <h1>Coordinate Tuples vs $F^n$ Vectors</h1>
        <ul>      
          <li step="0">Once a basis is picked, every vector is represented by
            a tuple of coordinates.

          <li step="1">The tuples are elements of $F^n$, the cartesian product space of
            the scalar field.

          <li step="2">The field $F$ by its own axioms has addition and multiplication
            defined. The cartesian product naturally has addition and scalar
            multiplication &mdash; $F^n$ is a vector space too!

          <li step="3">This vector space is <b>isomorphic</b> to the original
            vector space, which often misleads us to think that
            vectors just <em>are</em> tuples of numbers.

          <li step="4">The mapping of the vectors to its coordinate tuples is called
            an <b>Isomorphism</b>. There are many such mappings, one for each
            basis of the vector space.

          <li step="5">Isomorphism is pervasive in Mathematics, and it's the
            reason we can often afford to be "sloppy" when we speak.
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-bt">
        <h1>Coordinate Tuples vs $F^n$ Vectors</h1>
        <div step="0">Exercise:</div>
        <ul>      
          <li step="1"><b>Exercise:</b> Define two different bases in $R^2$, and
            compute the coordinates of one vector in both bases.
        </ul>
      </div>
      
      <div class="slide build-focus-visible" group="theme1-bt">
        <h1>Basis Transformation</h1>
        <div step="0">Let's consider <b>two</b> different bases
          $\{\xvec{a}_{i}\}$ and $\{\xvec{b}_{i^\prime}\}$ in the same vector
          space.</div>

        <ul>
          <li step="1"><b>Notation:</b> Symbols with differently primed indices
            refer to different objects. Thus $\xvec{a}_{i}$ is a different
            vector from $\xvec{a}_{i^\prime}$ even for equal values of $i$ and
            $i^\prime$.
            
          <li step="2">Every vector $\xvec{v}$ of the vector space has
            coordinates in the first base $\{\xvec{a}_{i}\}$ as well as in the
            second base $\{\xvec{b}_{i^\prime}\}$ :

            $$\xvec{v} = v^{i} \xvec{a}_{i} = v^{i^\prime} \xvec{b}_{i^\prime}$$

          <li step="3,4">The vectors ${\xvec{b}_{i^\prime}}$ of the second basis
            too have coordinates in the first base ${\xvec{a}_{i}}$. Let's call
            $T_{i^\prime}^{i}$ the coordinates of $\xvec{b}_{i^\prime}$ in the
            basis ${\xvec{a}_{i}}$:

            $$\xvec{b}_{i^\prime} = T_{i^\prime}^{i} \xvec{a}_{i}$$

          <li step="4"><b>Notation:</b> We could write $T$ as a matrix. But we
            don't, and stick to Einstein notation instead. We'll see later why
            &mdash; with tensors, will multiply such objects on "more than two
            sides".
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-bt">
        <h1>Basis Transformation</h1>
        <ul>
          <li step="0,1">Conversely, the vectors of the first basis
            $\{\xvec{a}_{i}\}$ also have coordinates in the second basis
            $\{\xvec{b}_{i^\prime}\}$ just like every vector too:

            $$\xvec{a}_{i} = T_{i}^{i^\prime} \xvec{b}_{i^\prime}$$

          <li step="1"><b>Notation:</b> the indices on $T_{i}^{i^\prime}$ and
            $T_{i^\prime}^{i}$ are different, so the note from the previous
            slide applies, and these are different objects.
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-bt">
        <h1>Basis Transformation</h1>
        <ul>
          <li step="0">So the two sets of coordinates $T_{i}^{i^\prime}$ and
            $T_{i^\prime}^{i}$ are different, but they are related, as we see
            from inserting one equation into the other:

            $$\xvec{a}_{i} = T_{i}^{i^\prime} \xvec{b}_{i^\prime}$$

            $$\xvec{a}_{i} = T_{i}^{i^\prime} T_{i^\prime}^{j} \xvec{a}_{j}$$

          <li step="1,2,3">thus:

            $$T_{i}^{i^\prime} T_{i^\prime}^{j} = \delta_{i}^{j}

            \;\;\mathrm{where}\;\;

            \delta_{i}^{j} = 1 \; (i=j), \; 0 \; (i \neq j)$$

          <li step="2">The two matrices $T_{i}^{i^\prime}$ and
            $T_{i^\prime}^{j}$ are each other's inverse.

          <li step="3"><b>Notation:</b> we don't have to care about transpose or
            ordering of factors.
        </ul>
      </div>
      
      <div class="slide build-focus-visible" group="theme1-bt">
        <h1>Basis Transformation</h1>
        <ul>
          <li step="0">We can ask about the relationship of the coordinates of a
            vector too:
            
            $$\xvec{v} = v^{i} \xvec{a}_{i} = v^{i^\prime} \xvec{b}_{i^\prime}$$ 

            $$\xvec{v} = v^{i} \xvec{a}_{i} = v^{i^\prime} T_{i^\prime}^{i} \xvec{a}_{i}$$ 

          <li step="1,2">thus:

            $$v^{i} = T_{i^\prime}^{i} v^{i^\prime}

            \;\; \mathrm{and\;remember} \;\;\ \xvec{a}_{i} = T_{i}^{i^\prime} \xvec{b}_{i^\prime}$$

          <li step="2">... basis vectors and coordinates transform inverse (and
            transposed) to each other.
          
          <li step="3"><b>Notation:</b> again ordering of factors is not
            important for meaning, unlike in matrix notation.
        </ul>
      </div>
          
      <div class="slide build-focus-visible" group="theme1-bt">
        <h1>Basis Transformation</h1>
        <ul>
          <li step="0"><b>Definition:</b> $T$ is called a <b>basis
            transformation</b>, and correspondingly a <b>coordinate
            transformation</b>.
            
          <li step="1"><b>Exercise:</b> Write down the basis transformation and
            the coordinate transformation for a rotation of 90 degrees clockwise
            in space of arrows in the 2D plane.
          
          <li step="2"><b>Exercise:</b> Write down the basis transformation and
            the coordinate transformation for a change of basis from a
            rectangular to a 45 degrees basis in the space of arrows in the 2D
            plane.
        </ul>
      </div>
      
      <div class="slide build-focus-visible" group="theme1-sp">
        <h1>Scalar Product and Metric Tensor</h1>
        <div>Let's look at the scalar product in Vector Spaces.</div>
        <ul>
          <li step="0">The scalar product is a map from the vector space to its
            scalar field:

            $$\xvec{v} \cdot \xvec{w} = a \in F$$

          <li step="1">linear:

            $$(a \xvec{v}) \cdot \xvec{w} = a (\xvec{v} \cdot \xvec{w})$$
            
            $$(\xvec{v} + \xvec{u}) \cdot \xvec{w} = \xvec{v} \cdot \xvec{w} + \xvec{u} \cdot \xvec{w}$$
            
          <li step="2">commutative:
            $\xvec{v} \cdot \xvec{w} = (\xvec{w} \cdot \xvec{v})$
            
          <li step="3">regular:
            $\forall \xvec{v} \neq \xvec{0} \; \exists \xvec{w}: \xvec{v} \cdot
            \xvec{w} \neq 0$
        </ul>

        <div class="notes">
          <ul>
            <li>For vector spaces over <b>Complex Numbers</b>, the scalar
              product is hermitean instead of commutative.
            <li>For "normal" vector spaces, the scalar product is also required
              to be positive definite.
          </ul>
        </div>
      </div>
            
      <div class="slide build-focus-visible" group="theme1-sp">
        <h1>Scalar Product and Metric Tensor</h1>
        <div step="0">We are on the way to define the first tensor.</div>
        <ul>
          <li step="1">The vectors can be written with coordinates in a basis ${\xvec{e}_{i}}$:

            $$\xvec{v} \cdot \xvec{w} = (v^{i}\xvec{e}_{i}) \cdot (w^{j} \xvec{e}_{j})$$

          <li step="2">... using the linearity of the scalar product yields:

            $$\xvec{v} \cdot \xvec{w} = v^{i} w^{j} (\xvec{e}_{i} \cdot \xvec{e}_{j})$$

          <li step="3">The scalar products of the basis vectors are just
            numbers. We call them $g$:

            $$g_{ij} := \xvec{e}_{i} \cdot \xvec{e}_{j}$$

          <li step="4">... then:

            $$\xvec{v} \cdot \xvec{w} = v^{i} w^{j} g_{ij}$$
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-sp">
        <h1>Scalar Product and Metric Tensor</h1>
        <ul>
          <li step="0">For a given basis, these numbers $g_{ij}$ fully define the scalar
            product in the vector space; they are the <em>coordinates</em> of
            the bilinear map that is the scalar product.


          <li step="1">We call them coordinates because they work like coordinates of
            vectors: The object they describe is not those numbers, it's a map
            of vectors to scalar numbers, just like the vectors are not their
            coordinates. But once vectors are described by coordinates relative
            to a basis, the map is described by coordinates too

          <li step="2"><b>Exercise:</b> Write down the coordinates of the scalar
            product for arrows in 2D space in a basis of your choice.
        </ul>
      </div>
      
      <div class="slide" group="theme1-sp">
        <h1>Scalar Product and Metric Tensor</h1>
        <div>Two interesting questions arise, both of which lead to the definition of tensors:</div>
        <ul>
          <li>(1) What are the coordinates of the scalar product in another
          basis?

          <li>(2) What are the "basis vectors" of whom the coordinates are the
            coefficients?
        </ul>

        <div><b>Exercise:</b> Discuss the questions above.</div>
      </div>

      <div class="slide build-focus-visible" group="theme1-sp">
        <h1>Scalar Product and Metric Tensor</h1>
        <div step="0">What are the coordinates of the scalar product in another
          basis? Lets see ...</div>
        <ul>
          <li step="1">$\xvec{v} \cdot \xvec{w} = (v^{i}\xvec{e}_{i}) \cdot (w^{j} \xvec{e}_{j})$
            
          <li step="2">$\xvec{v} \cdot \xvec{w} = (v^{i^\prime}\xvec{e}_{i^\prime}) \cdot (w^{j^\prime} \xvec{e}_{j^\prime})$

          <li step="3">$\xvec{v} \cdot \xvec{w} = (v^{i^\prime} T_{i^\prime}^{i} \xvec{e}_{i}) \cdot (w^{j^\prime} T_{j^\prime}^{j} \xvec{e}_{j})$

          <li step="4">$\xvec{v} \cdot \xvec{w} = v^{i^\prime} w^{j^\prime} \;
            T_{i^\prime}^{i} T_{j^\prime}^{j} \; (\xvec{e}_{i} \cdot \xvec{e}_{j})$

          <li step="5">$\xvec{v} \cdot \xvec{w} = v^{i^\prime} w^{j^\prime} \;
            T_{i^\prime}^{i} T_{j^\prime}^{j} \; g_{ij}$

          <li step="6">$\xvec{v} \cdot \xvec{w} = v^{i^\prime} w^{j^\prime} \;
            g_{i^{\prime}j^{\prime}}$

          <li step="7">thus, $g_{i^{\prime}j^{\prime}} = T_{i^\prime}^{i} T_{j^\prime}^{j} \; g_{ij}$

        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-sp">
        <h1>Tensors</h1>
        <div step="0">We can now give a definition of a Tensor, and of the Tensor
          Product Space.</div>
        <ul>
          <li step="1">A <b>Tensor</b> is any mathematical object in a vector space with
            coordinates that transform under basis transformations like a
            multilinear map.

          <li step="2">The scalar product is just one such map. There are many others.

          <li step="3">These maps be combined and multiplied with numbers:
            
            $$(g + h)(\xvec{a}, \xvec{b}) := g(\xvec{a}, \xvec{b}) + h(\xvec{a},
            \xvec{b})$$
            
            $$(ag)(\xvec{a}, \xvec{b}) := a \cdot g(\xvec{a}, \xvec{b})$$

          <li step="4">Looking closely, they form a vector space too!

          <li step="5">The <b>Tensor Product Space</b> of $V$ is the vector space
            comprised of tensors in $V$:

            $$g,h \in V \otimes V$$

          <li step="6">Linear maps with more than two arguments are from tensor
            product spaces with more than two factors:

            $$V \otimes V \otimes V \otimes ...$$

        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-sp">
        <h1>Scalar Product and Metric Tensor</h1>
        <div step="0">Now that we have defined Tensors, a few more Definitions.</div>
        <ul>
          <li step="1">A vector space over real numbers with a scalar product is
              a <b>Euclidean Vector Space</b>.
            
          <li step="2">Two vectors from a Euclidean Vector Space whose scalar product is
            $0$ are <b>Orthogonal</b>.

          <li step="3">If the scalar product is positive definite (not always the case),
            then he scalar product also gives rise to a <b>Norm</b>:

            $$|\xvec{x}| = \sqrt{\xvec{x} \cdot \xvec{x}}$$

          <li step="4">The <b>Angle</b> $\phi$ between two vectors $\xvec{x}$ and
            $\xvec{y}$ is defined by

            $$\cos \phi = \frac{\xvec{x} \cdot \xvec{y}}{\sqrt{\xvec{x} \cdot
            \xvec{x}} \sqrt{\xvec{y} \cdot \xvec{y}}}$$
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-sp">
        <h1>Scalar Product and Metric Tensor</h1>
        <ul>
          <li step="0">Because the scalar product defines both distance and
            direction in a Euclidean Vector Space, i.e. the <em>Metric</em>, and
            is a tensor, it's called the <b>Metric Tensor</b>.

          <li step="1">In <b>Riemann Geometry</b>, the metric tensor is in every
            point in space (i.e. it's a tensor field). In <b>General
            Relativity</b>, the metric tensor is connected to the mass by a
            partial differential equation.
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-sp">
        <h1>Higher Order Tensors</h1>
        <ul>
          <li step="0"><b>Tensor Product Spaces</b> of $V$ arise from multilinear maps in
            the same way, e.g.:
            
            $$V \otimes V \otimes V \otimes V$$

          <li step="1">Such tensors have coordinates with as many indices:

            $$g_{ijkl}$$

          <li step="2">that transform under a basis transformation:

            $$g_{i^{\prime}j^{\prime}k^{\prime}l^{\prime}} =
            T_{i^\prime}^{i}
            T_{j^\prime}^{j}
            T_{k^\prime}^{k}
            T_{l^\prime}^{l}
            g_{ijkl}$$

          <li step="3">Natural question: What's the dimension of that space, and what
            is its basis?
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-sp">
        <h1>Tensor Bases</h1>
        <div step="0">We answer the second of those questions first, and the
          first in the next session ...</div>
        <ul>
          <li step="1">Given a basis ${\xvec{e}_i}$ in $V$, we define a basis in
            $V \otimes V$ as follows:

            $${\xvec{e}^i \otimes \xvec{e}^j}$$

          <li step="2">is the bilinear function of two vectors from $V$ such that

            $$(\xvec{e}^i \otimes \xvec{e}^j)(\xvec{e}_k, \xvec{e}_l) = \delta^i_k
            \delta^j_l$$

          <li step="3">then a bilinear map $g$ with coordinates $g_{ij}$ is given by

            $$g = g_{ij} \, \xvec{e}^i \otimes \xvec{e}^j$$
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-sp">
        <h1>Tensor Bases</h1>
        <ul>
          <li step="0">
            $g(\xvec{x}, \xvec{y}) = g_{ij} (\xvec{e}^i \otimes \xvec{e}^j)(x^k
            \xvec{e}_k, y^l \xvec{e}_l)$

          <li step="1">
            $g(\xvec{x}, \xvec{y}) = g_{ij} x^k y^l (\xvec{e}^i \otimes \xvec{e}^j)(\xvec{e}_k, \xvec{e}_l)$

          <li step="2">
            $g(\xvec{x}, \xvec{y}) = g_{ij} x^k y^l \delta^i_k \delta^j_l$

          <li step="3">
            $g(\xvec{x}, \xvec{y}) = g_{ij} x^i y^j$

          <li step="4">
            as we said earlier.
            
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1">
        <h1>Summary</h1>
        <ul>
          <li step="0">Vectors can be added and multiplied.
          <li step="1">From that <b>follows</b> they have coordinates wrt a basis.
          <li step="2">Tensors are multilinear functions of vectors.
          <li step="3">They are themselves vectors (can be added and
            multiplied), and thus have coordinates wrt a basis.
          <li step="4">The basis and coordinates of the tensors are related to
            the basis of and coordinates of the vectors they are functions of.
          <li step="5">The coordinates of tensors are indexed by tuples of
            indexes, one from each tensor product factor. Hence the name tensor
            in software engineering for muli-dimensional arrays.
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1">
        <h1>Conclusion</h1>
        <div step="0">Back to the question we started with: Vector and tensor
          data structures in software engineering.</div>
        <ul>
          <li step="1">Data structures that are collections indexed by integers
            are known as <b>vectors</b> and in the tensorflow library
            as <b>tensors</b>.
          <li step="2">However, if for such collections there is no addition
            defined and no multiplication by scalars, then they are not vectors
            in the linear algebra sense.
          <li step="3">They took their name because of the important property of
            vectors to be described by tuples of scalar numbers, called
            coordinates.
        </ul>
      </div>
      
      <div class="slide" group="theme1">
        <h1>Next</h1>
        <ul>
          <li>We look at the dimension of tensor product spaces, and how tensor
            products appear in Quantum Computing, but also in data structures in
            Software Engineering.
        </ul>
      </div>

      <div class="slide"></div>
    </div>

  </body>
</html>
