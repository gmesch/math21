<!doctype html>
<html>
  <head>
    <title>Math for ML</title>
    
    <script src="lib/jslib.js"></script>    
    <script src="lib/state.js"></script>
    <script src="lib/state_util.js"></script>
    <script src="lib/slides.js"></script>

    <link href="lib/slides.css" rel="stylesheet">    
    <link href="slides_style.css" rel="stylesheet">

    <script src="slides_mathjax.js"></script>
    <script src="slides_loader.js"></script>
    <script src="prettify/run_prettify.js?autorun=false&lang=scm" async></script>

    <script src="lib/firebase.js"></script>
    <script src="lib/canvas.js"></script>
    <script src="slides_init.js"></script>
  </head>

  <body onload="init()">
    
    <div id="bgpage" class="background">
      <canvas id="canvas"></canvas>
    </div>
    
    <div class="header">
      <div id="follow" style="display:none">follow</div>
    </div>

    <div id="footer" class="footer">
      <div id="navigator" class="large"></div>
    </div>

    <div id="slides" class="slides">
      <div class="slide title"
           style="background-color:black; padding:5vw"
           onshow="d0('navigator','footer');v0('bgpage')"
           onhide="d1('navigator','footer');v1('bgpage')">
        <div style="height: 15%"></div>
        <h1 style="color:#ffa540">Welcome to Math for Machine Learning!</h1>
        <h1 style="color:#ffffff">Linear Algebra, Session
          6 &mdash; Tensors</h1>
        <h1 style="color:#808080">Mesch</h1>
        
        <h2 style="position:absolute; bottom:5%; padding-left:0.1vw; color:red">X/Google, 2021</h2>
      </div>

      <div class="slide noprint banner" group="theme2" name="theme2">
        <h1>Dimension of Tensor Product Spaces</h1>
        <div>
          <em>How dimension of tensor spaces grows with the number of factor
            spaces, and the consequences for Quantum Computing.</em>
        </div>
      </div>
      
      <div class="slide build-focus-visible" group="theme2">
        <h1>Dimension of Tensor Product Spaces - Overview</h1>
        <div>Plan for today.</div>
        <ol>
          <li step="1">Recap Quantum Mechanics

          <li step="2">Qubits, the simplest possible quantum systems

          <li step="3">Tensor products of qubits

          <li step="4">Quantum circuits for multiple qubits

          <li step="5">Wrap up
        </ol>
      </div>
      
      <div class="slide build-focus-visible" group="theme2">
        <h1>Recap</h1>
        <div>Scalar product spaces.</div>
        <ul>
          <li step="1">Vector space equipped with scalar product.
          <li step="2">Why not always deal in orthonormal bases? - In QM we will.
          <li step="3,5">Covector space basis can be defined as
            $$\xvec{e}^i(\xvec{e}_j) = \delta^i_j$$ <span step="5">$N$
            equations per basis vector.</span>
          <li step="4,5">Scalar product gives mapping of vectors and covectors
            $$\xvec{f}(\xvec{x}) := \xvec{f} \cdot \xvec{x}$$ <span step="5">One
            equation per vector.</span>
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme2">
        <h1>Quantum Mechanics</h1>
        <div>Postulates. Realization in Physics is more complicated.</div>
        <ol>
          <li step="0">Any physical system is associated with a Hilbert space
            (vector space with scalar product) aka the <b>state space</b>. The
            <b>state</b> of this system is a unit vector $\psi$ from the state
            space.

          <li step="1">Time evolution of a <em>closed</em> physical system is a unitary
            transformation of the state vector $U\psi$.

          <li step="2">Measurements of values $m$ of a physical property correspond to a
            set of operators ${M_m}$.

          <li step="3">Probability to measure value $m$ in the
            system is $$p(m) = M_m \psi \cdot M_m \psi$$.

          <li step="4">State after measurement is $$M_m\psi$$ normalized to unit
          vector.

          <li step="5">The measurement operators are complete $\sum_m
            M_m^\dagger M_m = 1$.
        </ol>
      </div>

      <div class="slide build-focus-visible" group="theme2">
        <h1>Dirac Notation</h1>
        <div>Used in quantum mechanics.</div>
        <ul>
          <li step="0">Vector space in QM is functions that are solutions to the
            Schr&ouml;dinger equation.
          <li step="1">Complex valued time dependent or time independent
            functions of 3d space (aka, fields) form a vector space. (Check
            axioms.)
          <li step="2">Have a natural scalar product. (Check axioms.)
          <li step="3">Schr&ouml;dinger operator is linear, so solutions of the
            equation for one eigenvalue (next session) form a subspace.
          <li step="4">In QM, every vector worth mentioning is the eigenvector
            of some operator.
          <li step="5">Idea in Dirac notation is to always label vectors with
            eigenvalues of some operator, and use those as "coordinates".
        </ul>
      </div>

      <div class="slide build-focus-visible continued" group="theme2">
        <h1>Dirac Notation</h1>
        <table>
          <tr>
            <th>
            <th style="text-align:left">Dirac notation
            <th style="text-align:left">Einstein notation
          </tr>
          <tr step="1">
            <td>Any vector
            <td>$\ket{\psi} \in V$
            <td>$\xvec{x} \in V$
          </tr>
          <tr step="2">
            <td>Vector basis
            <td>$\sum_i \psi_i \ket{i}$
            <td>$x^i \xvec{e}_i$
          </tr>
          <tr step="3">
            <td>Vector coordninates
            <td>$\psi_i = \braket{i|\psi}$
            <td>$x^i \xvec{e}_i$
          </tr>
        </table>
      </div>

      <div class="slide build-focus-visible continued" group="theme2">
        <h1>Dirac Notation</h1>
        <table>
          <tr>
            <th>
            <th step="1,2,3" style="text-align:left">Dirac notation
            <th step="1,2" style="text-align:left">Einstein notation
          </tr>
          <tr>
            <td step="1,3">Covector applied to vector
            <td step="1,3">$\braket{\phi|\psi} = \sum_{ij} \phi_i^\ast \psi_j \braket{i|j}
              = \sum_i \phi_i^\ast \psi_i$
            <td step="1">$\xvec{x} (\xvec{y}) = x_i y^j \xvec{e}^i(\xvec{e}_j)$
          </tr>
          <tr>
            <td step="2,3">Scalar product
            <td step="2,3">$\braket{\phi|\psi} = \sum_{ij} \phi_i^\ast \psi_j \braket{i|j}
              = \sum_i \phi_i^\ast \psi_i$
            <td step="2">$\xvec{x} \cdot \xvec{y} = x^i y^j \xvec{e}_i \cdot \xvec{e}_j = x^i y^j g_{ij}$
          </tr>
        </table>
      </div>

      <div class="slide build-focus-visible continued" group="theme2">
        <h1>Dirac Notation</h1>
        <table>
          <tr>
            <th>
            <th style="text-align:left">Dirac notation
            <th style="text-align:left">Einstein notation
          </tr>
          <tr>
            <td step="1">Operators
            <td step="1">$\ket{\psi}^\prime = A \ket{\psi}$
            <td step="1">$\xvec{x}^\prime = A \xvec{x}$
          </tr>
        </table>
      </div>

      <div class="slide build-focus-visible continued" group="theme2">
        <h1>Dirac Notation</h1>
        <div>Operator coordinates</div>
        <ul>
          <li step="1">$\ket{\psi}^\prime = A \ket{\psi}$
          <li step="2">$\psi^\prime_i = \sum_{j} A_{ij} \psi_j$
          <li step="3">$A_{ij} = \braket{i|A|j}$
          <li step="4">How can we see this:
          <li step="5">$\ket{\psi}^\prime = A \ket{\psi}$
          <li step="6">$\ket{\psi}^\prime = \sum_j A \ket{j} \braket{j|\psi}$
          <li step="7">$\braket{i|\psi}^\prime = \bra{i} \sum_j A \ket{j} \braket{j|\psi}$
          <li step="8">$\braket{i|\psi}^\prime = \sum_j \bra{i} A \ket{j} \braket{j|\psi}$
          <li step="9">$\psi^\prime_j = \sum_{i} A_{ij} \psi_i$
        </ul>
      </div>
        <div class="slide build-focus-visible" group="theme2">
        <h1>Qubits</h1>
        <div>Qubits are the algebraically simplest quantum systems.</div>
        <ul>
          <li step="1">Two dimensional state space, spanned by $\ket{0}$ and
            $\ket{1}$.
          <li step="2">Scalar product $\braket{i|j} = \delta_{ij}$.
          <li step="3">Thus $\ket{0}$ and
            $\ket{1}$ is an orthonormal basis.
          <li step="4">Consider only other orthonormal bases, thus all coordinate
            transformations are unitary.
          <li step="5">A qubit can be in any state $\psi_0 \ket{0} + \psi_1
            \ket{1}$
          <li step="6">Evolution is described by application of unitary
            operators $U$. Unitary operators are given by coordinates $U_{ij}$:
            <table>
              <tr>
                <td>
                  $$\ket{\psi^\prime} = U\ket{\psi}$$
                <td>
                  $$\ket{\psi^\prime} = \sum_i \psi^\prime_i \ket{i}$$
                <td>
                  $$\psi^\prime_i = \sum_j U_{ij} \psi_j$$
              </tr>
            </table>
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme2">
        <h1>Quantum Computation</h1>
        <div>Quantum computation is the manipulation of qubits with
          computational operators.</div>
        <ul>
          <li step="1">The point is to do this for more than qubits at a time.
          <li step="2">What is the state of a quantum computer with multiple qubits?
          <li step="3">The tensor product of multiple qubits.
          <li step="4">What is an operation of a computer?
          <li step="5">A unitary operator (which evolves the system) applied to the
            state.
          <li step="6">Also called <b>quantum gate</b>.
      </div>

      <div class="slide build-focus-visible continued" group="theme2">
        <h1>2 Qubits</h1>
        <ul>
          <li step="0">Two qubit basis:
            $$\ket{00} \ket{01} \ket{10} \ket{11}$$
          <li step="1">The system is always in a linear combination of these basis
            states.
          <li step="2">4 dimensional state space
          <li step="3">Unitary operators are described in terms of the effect on each
            of the base states, as usual, i.e. as a 4x4 matrix.
        </ul>
      </div>
      
      <div class="slide build-focus-visible continued" group="theme2">
        <h1>3 Qubits</h1>
        <ul>
          <li step="0">Three qubit basis:
            $$\ket{000} \ket{001} \ket{010} \ket{011}$$
            $$\ket{100} \ket{101} \ket{110} \ket{111}$$
          <li step="1">The system is always in a linear combination of these basis
            states.
          <li step="2"><b>8</b> (not 6) dimensional state space
          <li step="3">Unitary operators are described in terms of the effect on each
            of the base states, as usual, i.e. as an <b>8x8</b> matrix (not 6x6).
        </ul>
      </div>
      
      <div class="slide build-focus-visible" group="theme2">
        <h1>Quantum Gates</h1>
        <div>Examples for quantum gates on 1,2,3 qubits</div>
        <ul>
          <li step="1">1 Qubit &mdash; Hadamard $H$:
            $$\ket{0} \rightarrow \frac{1}{\sqrt{2}}(\ket{0} + \ket{1})$$
            $$\ket{1} \rightarrow \frac{1}{\sqrt{2}}(\ket{0} - \ket{1})$$
        </ul>
      </div>
      <div class="slide build-focus-visible continued" group="theme2">
        <h1>Quantum Gates</h1>
        <div>Examples for quantum gates on 1,2,3 qubits</div>
        <ul>
          <li step="1">2 Qubits &mdash; Controlled Not:
            $$\ket{00} \rightarrow \ket{00}$$
            $$\ket{01} \rightarrow \ket{01}$$
            $$\ket{10} \rightarrow \ket{11}$$
            $$\ket{11} \rightarrow \ket{10}$$
        </ul>
      </div>
      <div class="slide build-focus-visible continued" group="theme2">
        <h1>Quantum Gates</h1>
        <div>Examples for quantum gates on 1,2,3 qubits</div>
        <ul>
          <li step="1">3 Qubits &mdash; Toffoli:
            $$\ket{000} \rightarrow \ket{000}$$
            $$\ket{001} \rightarrow \ket{001}$$
            $$\ket{010} \rightarrow \ket{010}$$
            $$\ket{011} \rightarrow \ket{011}$$
            $$\ket{100} \rightarrow \ket{100}$$
            $$\ket{101} \rightarrow \ket{101}$$
            $$\ket{110} \rightarrow \ket{111}$$
            $$\ket{111} \rightarrow \ket{110}$$
          <li step="2">
            $$\ket{x,y,z} \rightarrow \ket{x,y,z +_2 xy}$$
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme2">
        <h1>Quantum Parallellism</h1>
        <div>This is why Quantum Computing is interesting.</div>
        <table>
          <tr>
            <td step="1" rowspan="2">2 Qubits &mdash; integer function $f$:
              $$\begin{array}{l}
              \ket{00} \rightarrow \ket{0,f(0)} \\
              \ket{01} \rightarrow \ket{0,1 +_2 f(0)} \\
              \ket{10} \rightarrow \ket{1,f(1)} \\
              \ket{11} \rightarrow \ket{1,1 +_2 f(1)}
              \end{array}$$
              $$\ket{x,y} \rightarrow \ket{x,y +_2 f(x)}$$

            <td step="2">Apply this to:
              $$(H\ket{0}) \ket{0} = \frac{\ket{0} + \ket{1}}{\sqrt{2}} \ket{0}$$
          </tr>
          <tr>
            <td step="3">
              yields:
              $$\frac{1}{\sqrt{2}}(\ket{0, f(0)} + \ket{1, f(1)})$$
          </tr>
        </table>
      </div>
      <div class="slide build-focus-visible continued" group="theme2">
        <h1>Quantum Parallellism</h1>
        <div>This is why Quantum Computing is interesting:
          $$\frac{1}{\sqrt{2}}(\ket{0, f(0)} + \ket{1, f(1)})$$
        </div>
        <ul>
          <li step="1">This computes $f$ for all input values <em>simultaneously</em>.
          <li step="2">When done with $N$ argument qubits of an n-ary function, computes
            $f$ for $2^N$ values simultaneously.
          <li step="3">Root cause that quantum computing is expected to be more powerful
            than classical computing.
          <li step="4">Caveat: the function values cannot all be <em>measured</em>. But
            they can serve as input to subsequent operations.
        </ul>
      </div>
      
      <div class="slide build-focus-visible" group="theme2">
        <h1>Wrap up</h1>
        <ul>
          <li step="0">Tensor product spaces have dimension of the product of each of
            their factors.
          <li step="1">Used to describe multiple qubits in QC, and multiple particles
            (e.g., electrons in atoms and molecules) in QM.
          <li step="2">This is ultimately what underlies expected power of QC to solve in
            sub-exponential time what would take classical computing exponential
            time.
        </ul>
      </div>

      <div class="slide noprint banner" group="theme1" name="theme1">
        <h1>End</h1>
      </div>
      
      <div class="slide title"
           style="background-color:black; padding:5vw"
           onshow="d0('navigator','footer');v0('bgpage')"
           onhide="d1('navigator','footer');v1('bgpage')">
        <div style="height: 15%"></div>
        <h1 style="color:#ffa540">Welcome to Math for Machine Learning!</h1>
        <h1 style="color:#ffffff">Linear Algebra, Session
          5 &mdash; Tensors</h1>
        <h1 style="color:#808080">Mesch</h1>
        
        <h2 style="position:absolute; bottom:5%; padding-left:0.1vw; color:red">X/Google, 2021</h2>
      </div>
      
      <div class="slide build-focus-visible" group="intro" name="intro">
        <h1>Intro: Tensors!</h1>
        <div>The ML library we all use is called
          Tensorflow, after those things from Linear Algebra. But
          what <em>are</em> Tensors really?</div>
        <div>Important, is what they are!</div>
        <ul>
          <li step="1">In <b>General Relativity</b>, tensors describe
            Gravitation.
            
          <li step="2">In <b>Quantum Mechanics</b>, multi-particle states are
            the tensor products of single particle states.

          <li step="3">In <b>Quantum Computing</b>, each qubit is one factor of
            a tensor product.

          <li step="4">In <b>Machine Learning</b> ... well, the "tensors" in
            Tensorflow are only superficially connected with what they are in
            Mathematics &mdash; we'll explain how exactly! But tensor products
            appear in interesting places when optimizing weight matrices in
            large models.
        </ul>

        <div class="notes">Remember to look at the notes.</div>
      </div>

      <div class="slide build-focus-visible" group="intro" name="plan">
        <h1>Game Plan</h1>
        <div>Throughout the next 3 sessions:</div>

        <ul>
          <li step="1">We explain what tensors are, and why the name became
            associated with multidimensional array data structures in software
            engineering, especially in machine learning.
            
          <li step="2" link="theme2">We illustrate how tensor products of vector spaces have a
            peculiar scaling behavior in their dimension: Unlike the Cartesian
            product, which adds the dimensions of its factors, the tensor
            product multiplies. We also contemplate how confusing the fact

            $$ 2 + 2 = 2 \times 2 = 2^2 $$

            is for understanding what happens when we scale beyond 2.

          <li step="3" link="theme3">We finally look again how tensors and their
            products appear in machine learning models.
        </ul>
      </div>

      <div class="slide build-focus-visible" group="intro">
        <h1>Meta</h1>

        <div>Besides the improved understanding of ML and QC, there are a few
          tangential motivations that are inspiring in their own right, or for
          building software:</div>

        <ul>
          <li step="1">Axiomatic construction.

          <li step="2">Introduce more notation.

          <li step="3">Revisit previous lessons from new viewpoints. (Remember
            the Tensorflow code retreat.)

          <li step="4">Encounter important mathematical concepts, such as
            Isomorphism.
        </ul>
      </div>

      <div class="slide build-focus-visible" group="intro">
        <h1>How to follow along</h1>

        <div>This is an experiment. How to use these slides:</div>

        <ul>
          <li step="1">Find them
            at <a href="https://gmesch.github.io/math21/slides.html">gmesch.github.io/math21/slides.html</a>

          <li step="2">To follow the slides as shown by the presenter, append
            <b>&amp;follow=<em>X</em></b> to the URL, using the value
            for <b><em>X</em></b> announced by the presenter.

          <li step="3">The slides will ask to allow notifications; please allow
            it (this is needed even though no notifications will be shown).

          <li step="4">Press <b>N</b> to show/hide speaker notes.
            
          <li step="5">Press <b>1</b> to enter/leave drawing mode.

          <li step="6">Press <b>left</b> and <b>right</b> to go backwards and
            forwards in the presentation.

          <li step="7">If you navigate on your own, automatic following
            pauses. Press <b>P</b> to resume automatic following.
            
          <li step="8">All this may not work in your browser, your machine, or
            on your network connection. If so, please just follow on the video
            stream.
            
        </ul>
      </div>

      <div class="slide build-focus-visible" group="intro">
        <h1>How to ask questions</h1>

        <div>This is also an experiment. How to ask questions on VC:</div>

        <ul>
          <li step="1">If you like ask a question, or propose a solution to an
            exercise, join the GVC and (politely) speak up.

          <li step="2">Once question is asked, please go back to live stream, so
            we don't exhaust participant numbers.
        </ul>
      </div>

      <div class="slide noprint banner" group="theme1" name="theme1">
        <h1>Vectors and Tensors</h1>
        <div>
          <em>Why, in software, arrays with multiple integer indices are called
            tensors.</em>
        </div>
      </div>
      
      <div class="slide build-visible" group="theme1">
        <h1>Vectors and Tensors - Overview</h1>
        <div>Plan for today.</div>
        <ol>
          <li step="0">Recap the Axioms of Vector Space

          <li step="1">Einstein Notation
            
          <li step="2">Linear Independence <em>gives rise to Bases and Coordinates</em>
            
          <li step="3">Basis and Coordinate Transformations

          <li step="4">The Scalar Product <span step="5">$\leftarrow$ This is
              the first Tensor!</span>

          <li step="6">Tensor Coordinates and Basis Transformations &mdash;
            Definition of Tensors

          <li step="7">Wrap up
        </ol>
      </div>

      <div class="slide build-focus-visible" group="theme1-vs">
        <h1>Axioms of Vector Space</h1>
        <div>Recap what Vectors are, and Vector Spaces.</div>

        <ul>
          <li step="1"><b>Vectors</b> can be <b>added</b> to each other and
            <b>multiplied with scalar</b> numbers.
            
          <li step="2,3,4,5,6"><em>Scalar numbers</em> are elements of a <b>Field</b>,
            often
            <ul>
              <li step="3"><b>Rational Numbers</b> $Q$,
              <li step="4"><b>Real Numbers</b> $R$,
              <li step="5">or <b>Complex Numbers</b> $C$,
              <li step="6">but also <em>finite fields</em>,
                notably the <b>Galois Field</b> $GF(2)$.
            </ul>
        </ul>
      </div>

      <div class="slide build-focus-visible continued" group="theme1-vs">
        <h1>Axioms of Vector Space</h1>
        <div>Every set $V$ whose elements can be <b>added</b> to each other, and
          <b>multiplied</b> with elements from a field $F$, with the following
          properties, is a vector space.</div>

        <div step="1,2,3,4,5,6,7">Vector Addition:</div>
        <ol>
          <li step="2,7">is an operation from $V \times V$ on $V$:

            $\xvec{a} + \xvec{b} \in V$

          <li step="3,7">is associative:

            $(\xvec{a} + \xvec{b}) + \xvec{c} = \xvec{a} + (\xvec{b} +
            \xvec{c})$

          <li step="4,7">is commutative:

            $\xvec{a} + \xvec{b} = \xvec{b} + \xvec{a}$</li>

          <li step="5,7">has a neutral element:

            $\exists \xvec{0} \; \forall \xvec{a} \,:\, \xvec{a} + \xvec{0} =
            \xvec{a}$

          <li step="6,7">has an inverse:

            $\forall \xvec{a} \; \exists -\!\xvec{a} \,:\, \xvec{a} +
            (-\xvec{a}) = \xvec{0}$
        </ol>

        <div class="notes">
          <ul>
            <li>$\forall$ is implied where absent in the axioms.
          </ul>
        </div>
      </div>
      
      <div class="slide build-focus-visible continued" group="theme1-vs">
        <h1>Axioms of Vector Space</h1>
        <div>Every set $V$ whose elements can be <b>added</b> to each other, and
          <b>multiplied</b> with elements from a field $F$, with the following
          properties, is a vector space.</div>

        <div step="1,2,3,4,5,6,7">Scalar Multiplication:</div>
        <ol>
          <li step="2,7">is an operation from $F \times V$ on $V$:

            $a\xvec{a} \in V$
            
          <li step="3,7">is compatible with Field multiplication: $a (b \xvec{a}) = (a b) \xvec{a}$
          <li step="4,7">has a neutral element: $1 \xvec{a} = \xvec{a}$
          <li step="5,7">is distributive over vector addition: $a \xvec{a} + a \xvec{b} = a (\xvec{a} + \xvec{b})$
          <li step="6,7">is distributive over field addition: $a \xvec{a} + b \xvec{a} = (a + b) \xvec{a}$
        </ol>

        <div class="notes">
          <ul>
            <li>$\forall$ is implied where absent in the axioms.
            <li>technically in some terminolgy systems, scalar multiplication is
              not an operation but a function
          </ul>
        </div>
      </div>

      <div class="slide build-focus-visible" group="theme1-vs">
        <h1>Axioms of Vector Space - Examples</h1>
        <div>Two examples of what vector spaces are.</div>
        <ul>
          <li step="1">Example 1: Arrows over Rational Numbers.
            
          <li step="2">Example 2: Tuples of Real Numbers over Real Numbers.
        </ul>
      </div>

      <div class="slide build-focus" group="theme1-vs">
        <h1>Axioms of Vector Space - Example: Arrows</h1>
        <div>Arrows in the 2D plane are vectors, with their operations defined
          by Euclidean Geometry.</div>
        <div style="float:right">
          <ul>
            <li step="1,2,9">Vector Addition
            <li step="3,4,9">Scalar Multiplication - negation
            <li step="5,6,9">Scalar Multiplication - integers
            <li step="7,8,9">Scalar Multiplication - integer fractions
            <li step="9">Together: Vector operations over $Q$
          </ul>
        </div>
              
        <div class="img">
          <img class="img" src="img/vectors/vector-plus.png" step="1">
          <img class="img" src="img/vectors/vector-plus-result.png" step="2">
          <img class="img" src="img/vectors/vector-neg.png" step="3">
          <img class="img" src="img/vectors/vector-neg-result.png" step="4">
          <img class="img" src="img/vectors/vector-mult.png" step="5">
          <img class="img" src="img/vectors/vector-mult-result.png" step="6">
          <img class="img" src="img/vectors/vector-frac.png" step="7">
          <img class="img" src="img/vectors/vector-frac-result.png" step="8">
        </div>
        <div class="notes">
          <ul>
            <li>All the operations are defined directly by operations in
              Euclidean geometry, not by the corresponding operations on
              coordinates.
          </ul>
        </div>
      </div>

      <div class="slide build-focus-visible" group="theme1-vs">
        <h1>Axioms of Vector Space &mdash; Example: Tuples</h1>
        <div>Tuples of rational or real numbers are vectors.</div>
        <ul>
          <li step="1,3">$(1, 2) + (1, 2) = (1 + 1, 2 + 2) = (2, 4)$</li>
          <li step="2,3">$3 \times (1, 2) = (3 \times 3, 3 \times 2) = (3,
            6)$</li>
          <li step="3">Vector operations for tuples from $R^2$ over $R$.
        </ul>
      </div>
      
      <div class="slide build-focus-visible banner" group="exercise" name="exercise1">
        <h1>Axioms of Vector Space &mdash; Exercise</h1>
        <div>Are these two examples vector spaces?
          <div step="1">(a) Tuples of Rational Numbers over Real Numbers?</div>
          <div step="2">(b) Tuples of Real Numbers over Rational Numbers?</div>
          <div class="colab">(<a href="https://colab.research.google.com/drive/1_3JOAKP09ExBtJGEItyI-WV2JKuqI5yy#scrollTo=bbOvmXSUHBoB&line=3&uniqifier=1">colab</a>)</div>
        </div>
      </div>
      
      <div class="slide build-focus" group="exercise">
        <h1>Axioms of Vector Space &mdash; Solution</h1>
        <div>Are these two examples vector spaces?</div>

        <ul>
          <li step="1,3">(a) Tuples of Rational Numbers over Real Numbers?
            <ul>
              <li>No,
                violates first axiom of scalar multiplication.
            </ul>
          <li step="2,3">(b) Tuples of Real Numbers over Rational Numbers?
            <ul>
              <li>Tricky! Yes, but it's infinite-dimensional. We'll see how once we
                understand linear independence ...
            </ul>
        </ul>
      </div>
      
      <div class="slide build-focus-visible" group="theme1-li">
        <h1>Linear Independence</h1>
        <div step="0">The existence of <b>coordinates</b> follows from the
          axioms.</div>

        <ul>
          <li step="1">Vectors can be <b>linearly combined</b>, i.e. added
            up with weights $w_i$:

            $$\sum_{i} w_i \xvec{v}_i$$

          <li step="2">Can such combinations yield $\xvec{0}$ with coefficients
            that are not all $0$? (It's always possible with all coefficients 0,
            of course.)
            
          <li step="3">Vectors in the combination are then said to
            be <b>linearly dependent</b>.

          <li step="4">Otherwise they are <b>linearly indedendent</b>.
        </ul>

        <div class="notes">
          <ul>
            <li>This derivation leads to the introduction of coordinates.

            <li>It follows directly form the algebraic structure introduced by
              the axioms.

            <li>The vectors $\xvec{v}_i$ in the sum are really <b>multiple
              vectors</b> (one for each index), <b>not</b> the <b>coordinates of
              one vector</b> &mdash; we get to coordinates later!
          </ul>
        </div>
      </div>

      <div class="slide build-focus-visible banner" group="exercise" name="exercise2">
        <h1>Linear Independence &mdash; Exercise</h1>
        
        <div>Imagine sets of arrows that are
          <div step="1">(a) linearly independent,</div>
          <div step="2">(b) linearly dependent.</div>
          <div class="colab">(<a href="https://colab.research.google.com/drive/1_3JOAKP09ExBtJGEItyI-WV2JKuqI5yy#scrollTo=xuiUv3HRIcLu&line=5&uniqifier=1">colab</a>)</div>
        </div>
      </div>

      <div class="slide build-focus" group="exercise">
        <h1>Linear Independence &mdash; Solution</h1>
        
        <div>Imagine sets of arrows that are</div>

        <ul style="float:right">
          <li step="1">(a) linearly independent,
          <li step="2">(b) linearly dependent.
        </ul>

        <div class="img">
          <img class="img" src="img/vectors/vector-independent.png" step="1">
          <img class="img" src="img/vectors/vector-dependent.png" step="2">
        </div>
      </div>

      <div class="slide build-focus-visible" group="theme1-li">
        <h1>Einstein Notation</h1>
        <div>We introduce a notation for vector algebra that's very neat.</div>
        <ul>
          <li step="1">Convention:

            $$w^i \xvec{v}_i := \sum_{i} w_i \xvec{v}_i$$

          <li step="2">If an index $i$ appears in a multiplication expression both as
            upper index $w^i$ and as lower index $\xvec{v}_i$, then the sum over
            the range of the index is implied.

          <li step="3">Matrix multiplication can be written with it too:

            $$\xvec{a}^T \xvec{M} \xvec{b} = a^i b^j M_{ij}$$

          <li step="4">but it works with more than two indices as well:

            $$a^i b^j c^k M_{ijk}$$

          <li step="5">The latter is why it's the preferred notation for writing
            tensors, as we will see.
        </ul>
      </div>

      <div class="slide build-focus-visible continued" group="theme1-li">
        <h1>Einstein Notation</h1>
        <ul>
          <li step="0">No sum for an index on its own:
            
            $$\xvec{v}_i$$
            
            is simply a tuple of vectors.

          <li step="1">And:

            $$w^i$$
            
            is simply a tuple of weights, i.e. numbers.
          
          <li step="2">No sum for a repeated index on top or bottom:
            
            $$M_{ii}$$
            
            is the tuple of the diagonal elements of the matrix $M_{ij}$.

        </ul>
      </div>

      <div class="slide build-focus-visible continued" group="theme1-li">
        <h1>Einstein Notation</h1>
        <div step="0,4">One more bit of terminology:</div>
        <ul>
          <li step="1,4">Lower indices
            
            $$\xvec{v}_i$$
            
            are called <b>covariant</b> indices.

          <li step="2,4">Upper indices

            $$w^i$$
            
            are called <b>contravariant</b> indices.
          
          <li step="3,4">We will see why when we introduce basis and coordinate
            transformations.
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-li">
        <h1>Linear Independence</h1>
        <div class="notes">
          More analysis yields interesting properties of linear independence.
        </div>

        <ul>
          <li step="0">In each vector space, the maximum number of elements $\xvec{e}_i$
            in a set of linearly independent vectors is <b>fixed</b>.

          <li step="1,3">For every other vector $\xvec{v}$ added to such a set, there is a
            linear combination that yields $\xvec{0}$:

            $$v^{i}\xvec{e}_{i} + v^{0}\xvec{v} = \xvec{0}$$</li>

          <li step="2,3">Another way of saying this is that $\xvec{v}$ can be combined from
            $\xvec{e}_{i}$. With the same coefficients as above, and noticing that
            $v^{0}$ cannot be $0$:
            
            $$\xvec{v} = - \frac{v^{i}}{v^0} \xvec{e}_{i}$$
            
            (Einstein notation applies.)

          <li step="3"><b>Exercise:</b>
            <ul>
              <li>(a) why must be $v^{0} \neq 0$?
              <li>(b) Why would it be a problem otherwise?
            </ul>
        </ul>
      </div>


      <div class="slide build-visible continued" group="theme1-li">
        <h1>Linear Independence</h1>
        <div class="notes">
          One more property, and it all yields 3 important concepts.
        </div>

        <ul>
          <li step="0">Write more simply;
            
            $$\xvec{v} = v^{i} \xvec{e}_{i}$$

          <li step="1">Are these coefficients unique?

          <li step="2">It turns out yes they are!

          <li step="3">Proof follows, to show how such things work ...
        </ul>
      </div>

      <div class="slide build-focus-visible continued" group="theme1-li">
        <h1>Linear Independence</h1>
        <div step="0"><b>Proof</b> that the coefficients of any vector are unambiguous.</div>

        <ul>
          <li step="1">Assume there are two such sets of coefficients:

            $$\xvec{v} = v^{i} \xvec{e}_{i} = u^{i} \xvec{e}_{i}$$

          <li step="2">subtract one from the other:

            $$\xvec{v} - \xvec{v} = v^{i} \xvec{e}_{i} - u^{i} \xvec{e}_{i}$$

            $$\xvec{0} = (v^{i} - u^{i}) \xvec{e}_{i}$$

          <li step="3">Now remember that $\xvec{e}_i$ are linearly independent and that
            means $\xvec{0}$ can be combined <em>only</em> with <b>all</b>
            coefficients $0$.
        </ul>
      </div>

      <div class="slide build-focus-visible continued" group="theme1-li">
        <h1>Linear Independence</h1>

        <ul>
          <li step="0">Thus for all $i$:

            $$v^{i} - u^{i} = 0$$

            (No Einstein sum here.)

          <li step="1">Or:

            $$v^{i} = u^{i}$$

            QED.
            
          <li step="2">Thus, every vector in the vector space can be represented
            as a combination with unambiguous coefficients from any maximal set
            of lineary independent vectors.</li>
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-bt">
        <h1>Dimension, Basis, and Coordinates</h1>
        <div step="0">From the concept of Linear Indepdendece, we arrive
          at <b>3 Definitions</b>:</div>

        <ul>
          <li step="1">The maximum number of elements in a set of linearly
            independent vectors of a vector space is the <b>Dimension</b> of
            that vector space.
            
          <li step="2">Any such set itself is a <b>Basis</b> of that vector
            space.
            
          <li step="3">The coefficients in the linear combination of the basis
            vectors that yields a vector are the <b>Coordinates</b> of the
            vector in that basis.
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-bt">
        <h1>Isomorphism &mdash; Coordinate Tuples vs $F^n$ Vectors</h1>
        <div step="0">We clear up the "confusion" of coordinate tuples and vectors from
          $F^n$ over $F$.</div>
        <ul>      
          <li step="1">Once a basis is picked, every vector is represented by
            a tuple of coordinates.

          <li step="2">The tuples are elements of $F^n$, the cartesian product space of
            the scalar field.

          <li step="3">The field $F$ by its own axioms has addition and multiplication
            defined. The cartesian product naturally has addition and scalar
            multiplication &mdash; $F^n$ is a vector space too!

          <li step="4">This vector space is <b>isomorphic</b> to the original
            vector space, which often misleads us to think that
            vectors just <em>are</em> tuples of numbers.

          <li step="5">The mapping of the vectors to its coordinate tuples is called
            an <b>Isomorphism</b>. There are many such mappings, one for each
            basis of the vector space.

          <li step="6">Isomorphism is pervasive in Mathematics, and it's the
            reason we can often afford to be "sloppy" when we speak.
        </ul>
      </div>

      <div class="slide build-focus-visible banner" group="exercise" name="exercise3">
        <h1>Coordinate Tuples vs $F^n$ Vectors &mdash; Exercise</h1>
        <div>
          <div step="1">Define two different bases in $R^2$, and compute the
            coordinates of one vector $\xvec{v}$ in both bases.</div>
          <div class="colab"><a href="https://colab.research.google.com/drive/1_3JOAKP09ExBtJGEItyI-WV2JKuqI5yy#scrollTo=VZ-yOI8ENddV&line=1&uniqifeier=1">colab</a></div>
        </div>
      </div>
      
      <div class="slide build-focus-visible" group="exercise">
        <h1>Coordinate Tuples vs $F^n$ Vectors &mdash; Solution</h1>
        <div step="0,7">Define two different bases in $R^2$, and compute the
          coordinates of one vector $\xvec{v} = \left[ \begin{array}{c} 1 \\ 1
          \end{array} \right]$ in both bases.</div>

        <table class="eqn">
          <tr>
            <td step="1,3,7">Basis 1:
            <td step="4,6,7">Vector in Basis 1:
          </tr>
          <tr>
            <td step="1,3,7">
              $\{ \left[ \begin{array}{c} 1 \\ 3 \end{array} \right],
              \left[ \begin{array}{c} 3 \\ 1 \end{array} \right]
              \}$
              
            <td step="4,6,7">
              $\left[ \begin{array}{c} 1 \\ 1 \end{array} \right] =
              \frac{1}{4} \left[ \begin{array}{c} 1 \\ 3 \end{array} \right] + 
              \frac{1}{4} \left[ \begin{array}{c} 3 \\ 1 \end{array} \right]$
          </tr>
          <tr>
            <td step="2,3,7">Basis 2:
            <td step="5,6,7">Vector in Basis 2:
          </tr>
          <tr>
            <td step="2,3,7">
              $\{ \left[ \begin{array}{c} 1 \\ 0 \end{array} \right],
              \left[ \begin{array}{c} 0 \\ 1 \end{array} \right]
              \}$
            <td step="5,6,7">
              $\left[ \begin{array}{c} 1 \\ 1 \end{array} \right] =
              1 \left[ \begin{array}{c} 1 \\ 0 \end{array} \right] + 
              1 \left[ \begin{array}{c} 0 \\ 1 \end{array} \right]$
          </tr>
        </table>
      </div>
      
      <div class="slide build-focus-visible" group="theme1-bt">
        <h1>Basis Transformation</h1>
        <div step="0">Let's consider <b>two</b> different bases
          $\{\xvec{a}_{i}\}$ and $\{\xvec{b}_{i^\prime}\}$ in the same vector
          space.</div>

        <ul>
          <li step="1"><b>Notation:</b> Symbols with differently primed indices
            refer to different objects. Thus $\xvec{a}_{i}$ is a different
            vector from $\xvec{a}_{i^\prime}$ even for equal values of $i$ and
            $i^\prime$.
            
          <li step="2">Every vector $\xvec{v}$ of the vector space has
            coordinates in the first base $\{\xvec{a}_{i}\}$ as well as in the
            second base $\{\xvec{b}_{i^\prime}\}$ :

            $$\xvec{v} = v^{i} \xvec{a}_{i} = v^{i^\prime} \xvec{b}_{i^\prime}$$

          <li step="3,4">The vectors ${\xvec{b}_{i^\prime}}$ of the second basis
            too have coordinates in the first base ${\xvec{a}_{i}}$. Let's call
            $T_{i^\prime}^{i}$ the coordinates of $\xvec{b}_{i^\prime}$ in the
            basis ${\xvec{a}_{i}}$:

            $$\xvec{b}_{i^\prime} = T_{i^\prime}^{i} \xvec{a}_{i}$$

          <li step="4"><b>Notation:</b> We could write $T$ as a matrix. But we
            don't, and stick to Einstein notation instead. We'll see later why
            &mdash; with tensors, will multiply such objects on "more than two
            sides".
        </ul>
      </div>

      <div class="slide build-focus-visible continued" group="theme1-bt">
        <h1>Basis Transformation</h1>
        <ul>
          <li step="0,1">Conversely, the vectors of the first basis
            $\{\xvec{a}_{i}\}$ also have coordinates in the second basis
            $\{\xvec{b}_{i^\prime}\}$ just like every vector too:

            $$\xvec{a}_{i} = T_{i}^{i^\prime} \xvec{b}_{i^\prime}$$

          <li step="1"><b>Notation:</b> the indices on $T_{i}^{i^\prime}$ and
            $T_{i^\prime}^{i}$ are different, so the note from the previous
            slide applies, and these are different objects.
        </ul>
      </div>

      <div class="slide build-focus-visible continued" group="theme1-bt">
        <h1>Basis Transformation</h1>
        <ul>
          <li step="0">So the two sets of coordinates $T_{i}^{i^\prime}$ and
            $T_{i^\prime}^{i}$ are different, but they are related, as we see
            from inserting one equation into the other:

            $$\xvec{a}_{i} = T_{i}^{i^\prime} \xvec{b}_{i^\prime}$$

            $$\xvec{a}_{i} = T_{i}^{i^\prime} T_{i^\prime}^{j} \xvec{a}_{j}$$

          <li step="1,2,3">thus:

            $$T_{i}^{i^\prime} T_{i^\prime}^{j} = \delta_{i}^{j}

            \;\;\mathrm{where}\;\;

            \delta_{i}^{j} = 1 \; (i=j), \; 0 \; (i \neq j)$$

          <li step="2">The two matrices $T_{i}^{i^\prime}$ and
            $T_{i^\prime}^{j}$ are each other's inverse.

          <li step="3"><b>Notation:</b> we don't have to care about transpose or
            ordering of factors.
        </ul>
      </div>
      
      <div class="slide build-focus-visible continued" group="theme1-bt">
        <h1>Basis Transformation</h1>
        <ul>
          <li step="0">We can ask about the relationship of the coordinates of a
            vector too:
            
            $$\xvec{v} = v^{i} \xvec{a}_{i} = v^{i^\prime} \xvec{b}_{i^\prime}$$ 

            $$\xvec{v} = v^{i} \xvec{a}_{i} = v^{i^\prime} T_{i^\prime}^{i} \xvec{a}_{i}$$ 

          <li step="1,2">thus:

            $$v^{i} = T_{i^\prime}^{i} v^{i^\prime}

            \;\; \mathrm{and\;remember} \;\;\ \xvec{a}_{i} = T_{i}^{i^\prime} \xvec{b}_{i^\prime}$$

          <li step="2">Basis vectors and coordinates transform inversely to each other.
          
          <li step="3">Hence the <b>terminology</b>:
            <ul>
              <li>Coordinates
                have <em>upper</em> indices, and they transform <b>contravriant</b>
                to the basis vectors.
              <li>Basis vectors have <em>lower</em> indices, and are
                called <b>covariant</b>.
            </ul>
          
          <li step="4"><b>Notation:</b> again ordering of factors is not
            important for meaning, unlike in matrix notation.
        </ul>
      </div>
          
      <div class="slide build-focus-visible continued" group="theme1-bt">
        <h1>Basis Transformation</h1>
        <div>We now can <b>define</b>:</div>
        <ul>
          <li step="0">$T$ is called a <b>basis transformation</b>,
          <li step="1">and correspondingly a <b>coordinate transformation</b>.
        </ul>
      </div>
      
      <div class="slide build-focus-visible banner" group="exercise" name="exercise4">
        <h1>Basis Transformation &mdash; Exercise</h1>
        <div>
          <div step="1">Write down the basis transformation and the coordinate
            transformation for the transition from <em>Basis 1</em> to <em>Basis
              2</em> in the previous exercise.</div>
          <div class="colab">(<a href="https://colab.research.google.com/drive/1_3JOAKP09ExBtJGEItyI-WV2JKuqI5yy#scrollTo=3kk30hC3Wfok&line=2&uniqifier=1">colab</a>)</div>
        </div>
      </div>
      
      <div class="slide build-focus-visible" group="exercise">
        <h1>Basis Transformation &mdash; Solution</h1>
        <div>Write down the basis transformation and the coordinate
          transformation for the transition between <em>Basis 1</em>
          and <em>Basis 2</em> in the previous exercise.</div>
        <table class="eqn">
          <tr>
            <td step="1,4">Vector:
            <td step="2,4">Basis 1:
            <td step="3,4">Basis 2:
          </tr>
          <tr>
            <td step="1,4">
              $\xvec{v} = \left[ \begin{array}{c} 1 \\ 1 \end{array} \right]$
            <td step="2,4">
              $\begin{array}{c}
              \xvec{e}_1 = \left[ \begin{array}{c} 1 \\ 3 \end{array} \right] &&
              \xvec{e}_2 = \left[ \begin{array}{c} 3 \\ 1 \end{array} \right]
              \end{array}$

            <td step="3,4">
                $\begin{array}{c}
                \xvec{e}_{1^\prime} = \left[ \begin{array}{c} 1 \\ 0 \end{array}
                \right] &&
                
                \xvec{e}_{2^\prime} = \left[ \begin{array}{c} 0 \\ 1 \end{array}
                \right]
                
                \end{array}$
          </tr>
        </table>
      </div>

      <div class="slide build-focus-visible continued" group="exercise">
        <h1>Basis Transformation &mdash; Solution</h1>
        <div>Vector expressed in both bases:</div>
        <table class="eqn">
          <tr>
            <td step="0,2">Vector in Basis 1:
            <td step="1,2">Vector in Basis 2:                
          </tr>
          <tr>
            <td step="0,2">
              $\xvec{v} = v^i\xvec{e}_i = 
              \frac{1}{4} \left[ \begin{array}{c} 1 \\ 3 \end{array} \right] + 
              \frac{1}{4} \left[ \begin{array}{c} 3 \\ 1 \end{array} \right]$
            <td step="1,2">
              $\xvec{v} = v^{i^\prime}\xvec{e}_{i^\prime} = 
              1 \left[ \begin{array}{c} 1 \\ 0 \end{array} \right] + 
              1 \left[ \begin{array}{c} 0 \\ 1 \end{array} \right]$
          </tr>
          <tr>
            <td step="0,2">
              $v^1 = \frac{1}{4}, \; v^2 = \frac{1}{4}$
            <td step="1,2">
              $v^1 = 1, \; v^2 = 1$
          </tr>
        </table>
      </div>

      <div class="slide build-focus-visible continued" group="exercise">
        <h1>Basis Transformation &mdash; Solution</h1>
        <div step="0,6">Basis transformation written down:</div>
        <table class="eqn">
          <tr>
            <td step="1,6">
              $\xvec{e}_{i^\prime} = T_{i^\prime}^{i} \xvec{e}_{i}$
          </tr>
          <tr>
            <td step="2,6">
              $\xvec{e}_{1^\prime} = \left[ \begin{array}{c} 1 \\ 0 \end{array}
              \right] = 
              -\frac{1}{8} \left[ \begin{array}{c} 1 \\ 3 \end{array} \right] + 
              \frac{3}{8} \left[ \begin{array}{c} 3 \\ 1 \end{array} \right] =
              -\frac{1}{8} \xvec{e}_{1} +
              \frac{3}{8} \xvec{e}_{2}$
            <td step="3,6">
              $
              \begin{array}{c}
              T_{1^\prime}^{1} =-\frac{1}{8} &&
              T_{1^\prime}^{2} = \frac{3}{8} \\
              \end{array}$
          </tr>
          <tr>
            <td step="4,6">
              $\xvec{e}_{2^\prime} = \left[ \begin{array}{c} 0 \\ 1 \end{array}
              \right] =
              \frac{3}{8} \left[ \begin{array}{c} 1 \\ 3 \end{array} \right] -
              \frac{1}{8} \left[ \begin{array}{c} 3 \\ 1 \end{array} \right] =
              \frac{3}{8} \xvec{e}_{1} -
              \frac{1}{8} \xvec{e}_{2}$

            <td step="5,6">
              $\begin{array}{c}
              T_{2^\prime}^{1} = \frac{3}{8} &&
              T_{2^\prime}^{2} = -1\frac{1}{8}
              \end{array}$
          </tr>
        </table>
      </div>


      <div class="slide build-focus-visible continued" group="exercise">
        <h1>Basis Transformation &mdash; Solution</h1>
        <div step="0,9">Coordinate transformation written down:</div>
        <table class="eqn">
          <tr>
            <td step="1,9">
              $v^{i^\prime} = T_{i}^{i^\prime} v^{i}$
            <td step="2,5,8,9">
              $T_{i}^{i^\prime} T_{i^\prime}^{j} = \delta_i^j$
          </tr>
          <tr>
            <td step="3,9">
              $v^{1^\prime} = T_{1}^{1^\prime} v^{1} + T_{2}^{1^\prime} v^{2}$
              
            <td step="4,9">
              $1 = T_{1}^{1^\prime} \frac{1}{4} + T_{2}^{1^\prime} \frac{1}{4}$
              
            <td step="5,9">
              $\begin{array}{c}
              T_{1}^{1^\prime} = 1 &&
              T_{2}^{1^\prime} = 3
              \end{array}$
          </tr>
          <tr>
            <td step="6,9">
              $v^{2^\prime} = T_{1}^{2^\prime} v^{1} + T_{2}^{2^\prime} v^{2}$

            <td step="7,9">
              $1 = T_{1}^{2^\prime} \frac{1}{4} + T_{2}^{2^\prime} \frac{1}{4}$

            <td step="8,9">
              $\begin{array}{c}
              T_{1}^{2^\prime} = 3 && 
              T_{2}^{2^\prime} = 1
              \end{array}$
          </tr>
        </table>
      </div>
      
      <div class="slide build-focus-visible" group="theme1-sp">
        <h1>Scalar Product and Metric Tensor</h1>
        <div>Let's look at the scalar product in Vector Spaces.</div>
        <ul>
          <li step="0">The scalar product is a map from the vector space to its
            scalar field:

            $$\xvec{v} \cdot \xvec{w} = a \in F$$

          <li step="1">linear:

            $$(a \xvec{v}) \cdot \xvec{w} = a (\xvec{v} \cdot \xvec{w})$$
            
            $$(\xvec{v} + \xvec{u}) \cdot \xvec{w} = \xvec{v} \cdot \xvec{w} + \xvec{u} \cdot \xvec{w}$$
            
          <li step="2">commutative:
            $\xvec{v} \cdot \xvec{w} = \xvec{w} \cdot \xvec{v}$
            
          <li step="3">regular:
            $\forall \xvec{v} \neq \xvec{0} \; \exists \xvec{w}: \xvec{v} \cdot
            \xvec{w} \neq 0$
        </ul>

        <div class="notes">
          <ul>
            <li>For vector spaces over <b>Complex Numbers</b>, the scalar
              product is hermitean instead of commutative.
            <li>For "normal" vector spaces, the scalar product is also required
              to be positive definite.
          </ul>
        </div>
      </div>
            
      <div class="slide build-focus-visible continued" group="theme1-sp">
        <h1>Scalar Product and Metric Tensor</h1>
        <div step="0">We are on the way to define the first tensor.</div>
        <ul>
          <li step="1">The vectors can be written with coordinates in a basis $\{\xvec{e}_{i}\}$:

            $$\xvec{v} \cdot \xvec{w} = (v^{i}\xvec{e}_{i}) \cdot (w^{j} \xvec{e}_{j})$$

          <li step="2">... using the linearity of the scalar product yields:

            $$\xvec{v} \cdot \xvec{w} = v^{i} w^{j} (\xvec{e}_{i} \cdot \xvec{e}_{j})$$

          <li step="3">The scalar products of the basis vectors are just
            numbers. We call them $g$:

            $$g_{ij} := \xvec{e}_{i} \cdot \xvec{e}_{j}$$

          <li step="4">... then:

            $$\xvec{v} \cdot \xvec{w} = v^{i} w^{j} g_{ij}$$
        </ul>
      </div>

      <div class="slide build-focus-visible continued" group="theme1-sp">
        <h1>Scalar Product and Metric Tensor</h1>
        <div>
            $$\xvec{v} \cdot \xvec{w} = v^{i} w^{j} g_{ij}$$
        </div>
        <ul>
          <li step="0">For a given basis, these numbers $g_{ij}$ fully define the scalar
            product in the vector space; they are the <em>coordinates</em> of
            the bilinear map that is the scalar product.


          <li step="1">We call them coordinates because they work like coordinates of
            vectors.

          <li step="2">The object they describe <em>is not</em> those numbers,
            it's a map of vectors to scalar numbers, just like the vectors are
            not their coordinates.

          <li step="3">But once vectors are described by coordinates relative to
            a basis, the map is described by coordinates too.

          <li step="4">The coordinates of a bilinear map have <em>lower</em>
            indices. What does this mean for their coordinate transformations?
        </ul>
      </div>
      
      <div class="slide build-focus-visible banner" group="exercise" name="exercise5">
        <h1>Scalar Product and Metric Tensor &mdash; Exercise</h1>
        <div>
          <div step="1">Write down the coordinates of the scalar product for
            tuples from $R^2$ in the two bases from the previous
            exercise.</div>
          <div class="colab">(<a href="https://colab.research.google.com/drive/1_3JOAKP09ExBtJGEItyI-WV2JKuqI5yy#scrollTo=JTywKOH-P6ey&line=4&uniqifier=1">colab</a>)</div>
        </div>
      </div>

      <div class="slide build-focus-visible" group="exercise">
        <h1>Scalar Product and Metric Tensor &mdash; Solution</h1>
        <div step="0,10">Write down the coordinates of the scalar product for
          tuples from $R^2$ in the two bases from the previous exercise.</div>
        <table class="eqn">
          <tr>
            <td step="1,2,7,10">Vectors:
            <td step="3,4,8,10">Basis 1:
            <td step="5,6,9,10">Basis 2:
          </tr>

          <tr>
            <td step="2,10">

            $\begin{array}{c}

            \xvec{v} = \left[ \begin{array}{c} 1 \\ 1 \end{array} \right] &&

            \xvec{w} = \left[ \begin{array}{c} 1 \\ 0 \end{array} \right] 

            \end{array}$

            <td step="4,10">
            $\begin{array}{c}

            \xvec{e}_1 = \left[ \begin{array}{c} 1 \\ 3 \end{array} \right] &&

            \xvec{e}_2 = \left[ \begin{array}{c} 3 \\ 1 \end{array} \right]

            \end{array}$
            
          <td step="6,10">
            $\begin{array}{c}
            
            \xvec{e}_{1^\prime} = \left[ \begin{array}{c} 1 \\ 0 \end{array}
            \right] &&

            \xvec{e}_{2^\prime} = \left[ \begin{array}{c} 0 \\ 1 \end{array}
            \right]

            \end{array}$
          </tr>

          <tr>
            <td step="7,10">
              $\xvec{v} \cdot \xvec{w} = v^{i} w^{j} g_{ij} = v^{i^{\prime}} w^{j^{\prime}} g_{i^{\prime}j^{\prime}}$

            <td step="8,10">
              $g_{ij} := \xvec{e}_{i} \cdot \xvec{e}_{j}$

            <td step="9,10">
              $g_{i^{\prime}j^{\prime}} := \xvec{e}_{i^{\prime}} \cdot \xvec{e}_{j^{\prime}}$
          </tr>

        </table>
      </div>
      
      <div class="slide build-focus-visible continued" group="exercise">
        <h1>Scalar Product and Metric Tensor &mdash; Solution</h1>
        <div step="0,5">Write down the coordinates of the scalar product for
          tuples from $R^2$ in the two bases from the previous exercise.</div>
        <div style="display:flex">
          <div step="1,5">Basis 1:
            $$\begin{array}{c}

            \xvec{e}_1 = \left[ \begin{array}{c} 1 \\ 3 \end{array} \right] &&

            \xvec{e}_2 = \left[ \begin{array}{c} 3 \\ 1 \end{array} \right]

            \end{array}$$
          </div>
            
          <div step="2,5">Basis 2:
            $$\begin{array}{c}
            
            \xvec{e}_{1^\prime} = \left[ \begin{array}{c} 1 \\ 0 \end{array}
            \right] &&

            \xvec{e}_{2^\prime} = \left[ \begin{array}{c} 0 \\ 1 \end{array}
            \right]

            \end{array}$$
          </div>
        </div>

        <div step="3,5">Coordinates in Basis 1:
          $$\begin{array}{c}
          
          g_{11} = \xvec{e}_1 \cdot \xvec{e}_1 = 10 &&
          g_{12} = \xvec{e}_1 \cdot \xvec{e}_2 = 6 \\
          g_{21} = \xvec{e}_2 \cdot \xvec{e}_1 = 6 &&
          g_{22} = \xvec{e}_2 \cdot \xvec{e}_2 = 10
          
          \end{array}$$
        </div>
        
        <div step="4,5">Coordinates in Basis 2:
          $$\begin{array}{c}
          
          g_{1^{\prime}1^{\prime}} = \xvec{e}_{1^{\prime}} \cdot \xvec{e}_{1^{\prime}} = 1 &&
          g_{1^{\prime}2^{\prime}} = \xvec{e}_{1^{\prime}} \cdot \xvec{e}_{2^{\prime}} = 0 \\
          g_{2^{\prime}1^{\prime}} = \xvec{e}_{2^{\prime}} \cdot \xvec{e}_{1^{\prime}} = 0 &&
          g_{2^{\prime}2^{\prime}} = \xvec{e}_{2^{\prime}} \cdot \xvec{e}_{2^{\prime}} = 1
          
          \end{array}$$
        </div>
      </div>
      
      <div class="slide" group="theme1-sp">
        <h1>Scalar Product and Metric Tensor</h1>
        <div>Two interesting questions arise, both of which lead to the definition of tensors:</div>
        <ul>
          <li>(1) How do the coordinates of the scalar
            product <em>transform</em> to another basis?

          <li>(2) What are the <em>basis vectors</em> whose coordinates the
            coefficients are?
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-sp">
        <h1>Scalar Product and Metric Tensor &mdash; Coordinate Transformation</h1>
        <div step="0">What are the coordinates of the scalar product in another
          basis? Lets see ...</div>
        <ul>
          <li step="1">$\xvec{v} \cdot \xvec{w} = (v^{i}\xvec{e}_{i}) \cdot (w^{j} \xvec{e}_{j})$
            
          <li step="2">$\xvec{v} \cdot \xvec{w} = (v^{i^\prime}\xvec{e}_{i^\prime}) \cdot (w^{j^\prime} \xvec{e}_{j^\prime})$

          <li step="3">$\xvec{v} \cdot \xvec{w} = (v^{i^\prime} T_{i^\prime}^{i} \xvec{e}_{i}) \cdot (w^{j^\prime} T_{j^\prime}^{j} \xvec{e}_{j})$

          <li step="4">$\xvec{v} \cdot \xvec{w} = v^{i^\prime} w^{j^\prime} \;
            T_{i^\prime}^{i} T_{j^\prime}^{j} \; (\xvec{e}_{i} \cdot \xvec{e}_{j})$

          <li step="5">$\xvec{v} \cdot \xvec{w} = v^{i^\prime} w^{j^\prime} \;
            T_{i^\prime}^{i} T_{j^\prime}^{j} \; g_{ij}$

          <li step="6">$\xvec{v} \cdot \xvec{w} = v^{i^\prime} w^{j^\prime} \;
            g_{i^{\prime}j^{\prime}}$

          <li step="7">thus, $$g_{i^{\prime}j^{\prime}} = T_{i^\prime}^{i} T_{j^\prime}^{j} \; g_{ij}$$

        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-sp">
        <h1>Tensors</h1>
        <div step="0">We can now give a <b>definition</b> of
          a <b>Tensor</b>.</div>
        <ul>
          <li step="1">A <b>Tensor</b> is any mathematical object in a vector
            space with coordinates that transform under basis transformations
            like a multilinear map.

          <li step="2">The scalar product is just one such map. There are many
          others.

          <li step="3">These maps can be combined and multiplied with numbers:
            
            $$(g + h)(\xvec{a}, \xvec{b}) := g(\xvec{a}, \xvec{b}) + h(\xvec{a},
            \xvec{b})$$
            
            $$(ag)(\xvec{a}, \xvec{b}) := a \cdot g(\xvec{a}, \xvec{b})$$

          <li step="4">Looking closely, they form a vector space too!

          <li step="5">What vector space is that?
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-sp">
        <h1>Tensor Product Spaces</h1>
        <div step="0">We can now give a <b>definition</b> of the <b>Tensor
            Product Space</b>.</div>
        <ul>
          <li step="1">The <b>Tensor Product Space</b> of $V$ is the vector space
            comprised of tensors in $V$:

            $$g,h \in V \otimes V$$

          <li step="2">Linear maps with more than two arguments are from tensor
            product spaces with more than two factors:

            $$V \otimes V \otimes V \otimes ...$$

        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-sp">
        <h1>Scalar Product and Metric Tensor</h1>
        <div step="0">Now that we have defined Tensors, a few more <b>Definitions</b>.</div>
        <ul>
          <li step="1">A vector space over real numbers with a scalar product is
              a <b>Euclidean Vector Space</b>.
            
          <li step="2">Two vectors from a Euclidean Vector Space whose scalar product is
            $0$ are <b>Orthogonal</b>.

          <li step="3">If the scalar product is positive definite (not always the case),
            then he scalar product also gives rise to a <b>Norm</b>:

            $$|\xvec{x}| = \sqrt{\xvec{x} \cdot \xvec{x}}$$

          <li step="4">The <b>Angle</b> $\phi$ between two vectors $\xvec{x}$ and
            $\xvec{y}$ is defined by

            $$\cos \phi = \frac{\xvec{x} \cdot \xvec{y}}{\sqrt{\xvec{x} \cdot
            \xvec{x}} \sqrt{\xvec{y} \cdot \xvec{y}}}$$
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-sp">
        <h1>Scalar Product and Metric Tensor</h1>
        <ul>
          <li step="0">Because the scalar product defines both distance and
            direction in a Euclidean Vector Space, i.e. the <em>Metric</em>, and
            is a tensor, it's called the <b>Metric Tensor</b>.

          <li step="1">In <b>Riemann Geometry</b>, the metric tensor is given in
            every point in space, i.e. it's a <b>tensor field</b>.

          <li step="2">In <b>General Relativity</b>, the metric tensor is
            connected to the mass by a partial differential equation.
        </ul>

        <div class="notes">
          Don't confuse a tensor field with a field.
        </div>                                     
      </div>

      <div class="slide build-focus-visible" group="theme1-sp">
        <h1>Higher Order Tensors</h1>
        <ul>
          <li step="0"><b>Tensor Product Spaces</b> of $V$ arise from
            multilinear maps in the same way, e.g.:
            
            $$V \otimes V \otimes V \otimes V$$

          <li step="1">Such tensors have coordinates with as many indices:

            $$g_{ijkl}$$

          <li step="2">that transform under a basis transformation:

            $$g_{i^{\prime}j^{\prime}k^{\prime}l^{\prime}} =
            T_{i^\prime}^{i}
            T_{j^\prime}^{j}
            T_{k^\prime}^{k}
            T_{l^\prime}^{l}
            g_{ijkl}$$

          <li step="3">Natural question: What's the dimension of that space, and what
            is its basis?
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-sp">
        <h1>Tensor Bases</h1>
        <div step="0">We answer the second of those questions first, and the
          first in the next session ...</div>
        <ul>
          <li step="1">Given a basis ${\xvec{e}_i}$ in $V$, we define a basis in
            $V \otimes V$ as follows:

            $${\xvec{e}^i \otimes \xvec{e}^j}$$

          <li step="2">is the bilinear function of two vectors from $V$ such that

            $$(\xvec{e}^i \otimes \xvec{e}^j)(\xvec{e}_k, \xvec{e}_l) = \delta^i_k
            \delta^j_l$$

          <li step="3">then a bilinear map $g$ with coordinates $g_{ij}$ is given by

            $$g = g_{ij} \, \xvec{e}^i \otimes \xvec{e}^j$$
        </ul>
      </div>

      <div class="slide build-focus-visible continued" group="theme1-sp">
        <h1>Tensor Bases</h1>
        <ul>
          <li step="0">
            $g(\xvec{x}, \xvec{y}) = (g_{ij} \xvec{e}^i \otimes \xvec{e}^j)(x^k
            \xvec{e}_k, y^l \xvec{e}_l)$

          <li step="1">
            $g(\xvec{x}, \xvec{y}) = g_{ij} x^k y^l (\xvec{e}^i \otimes \xvec{e}^j)(\xvec{e}_k, \xvec{e}_l)$

          <li step="2">
            $g(\xvec{x}, \xvec{y}) = g_{ij} x^k y^l \delta^i_k \delta^j_l$

          <li step="3,4">
            $g(\xvec{x}, \xvec{y}) = g_{ij} x^i y^j$

          <li step="4">
            as we said earlier.
            
          <li step="5">
            Notice the basis vectors in a tensor product space
            have <em>covariant</em> indices.
      </div>

      <div class="slide build-focus-visible continued" group="theme1-sp">
        <h1>Tensor Bases</h1>
        <div step="0">Last question, what are those $\xvec{e}^i$ in $\xvec{e}^i
          \otimes \xvec{e}^j$?</div>
        <ul>
          <li step="1">There is a vector space of <em>linear
              functionals</em>. Its basis is defined by
            
            $$\xvec{e}^i(\xvec{e}_j) = \delta^i_j$$

          <li step="2">The linear functionals can be added and scalar multiplied
            too (sounds familiar by now ...) so they form a vector space too.

          <li step="3">It's called the <b>Co Vector Space</b> $\bar{V}$ of
            $V$. It's <em>isomorphic</em> to $V$.

          <li step="4">The vector space $V$ acts as the co vector space for its
            own co covector space. That's an example of <b>duality</b>.
            
          <li step="5">We can define tensors that are multilinear maps from both
            vectors and covectors to scalars. Those tensors have coordinates
            with both <em>covariant</em> and <em>contravariant</em> indices,
            e.g.:

            $$R^i_{jkl}$$

            (the curvature tensor in Riemann geometry).

          <li step="6">The metric tensor establishes a mapping between vectors
            and covectors: Every linear functional can be written as the scalar
            product of a vector with the vector the linear functional applies
            to.

        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1">
        <h1>Summary</h1>
        <ul>
          <li step="0"><b>Vectors</b> can be added and multiplied.
          <li step="1">From that <b>follows</b> they have coordinates with
            regard to a basis.
          <li step="2"><b>Tensors</b> are multilinear functions of vectors.
          <li step="3">Tensors are themselves vectors (can be added and
            multiplied), and thus have coordinates wrt a basis.
          <li step="4">The basis and coordinates of the tensors are related to
            the basis of and coordinates of the vectors they are functions of.
          <li step="5">The coordinates of tensors are indexed by tuples of
            indexes, one from each tensor product factor. Hence the
            name <em>tensor</em> in software engineering for muli-dimensional
            arrays. The multi-dimensional arrays are not themselves tensors, but
            the structure needed to represent the coordinated of tensors.
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1">
        <h1>Conclusion</h1>
        <div step="0">Back to the question we started with: Vector and tensor
          data structures in software engineering.</div>
        <ul>
          <li step="1">Data structures that are collections indexed by integers
            are known as <b>vectors</b> and in the tensorflow library
            as <b>tensors</b>.
          <li step="2">However, if for such collections there is no addition
            defined and no multiplication by scalars (and hence no basis, no
            basis transformation, and no coordinate transformation), then they
            are not vectors or tensors in the Linear Algebra sense.
          <li step="3">They took their name because of the important property of
            vectors and tensors to be described by tuples of scalar numbers,
            called coordinates.
        </ul>
      </div>
      
      <div class="slide" group="intro">
        <h1>Next</h1>
        <ul>
          <li>We look at the dimension of tensor product spaces, and how tensor
            products appear in Quantum Computing, but also in data structures in
            Software Engineering.
        </ul>
      </div>

      <div class="slide" group="intro"></div>
    </div>

  </body>
</html>
