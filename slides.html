<!doctype html>
<html>
  <head>
    <title>Math for ML</title>
    
    <script src="lib/jslib.js"></script>
    
    <script src="lib/slides.js"></script>
    <link href="lib/slides.css" rel="stylesheet">
    
    <script src="lib/state.js"></script>
    <script src="lib/state_util.js"></script>
    
    <script src="mathjax/tex-mml-chtml.js" async></script>
    <script src="prettify/run_prettify.js?autorun=false&lang=scm" async></script>

    <link href="fonts/fonts.css" rel="stylesheet">

    <script>
      MathJax = {
      tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      processEscapes: true,
      tags: 'ams'
      },
      chtml: {
      displayAlign: 'left',
      displayIndent: '1em'
      }
      };
    </script>
    
    <script>
      function init() {
        initslides();

        preAdjust();
        PR.prettyPrint();

        console.log('slides init done');
      }
    </script>

    <style>
      div.img { position: relative }
      img.img { position: absolute }

      #navigator div.intro { background-color: #ffffff; }
      #navigator div.theme1 { background-color: #e0ffff; }
      #navigator div.theme1-vs { background-color: #e0ffff; }
      #navigator div.theme1-li { background-color: #e0efef; }
      #navigator div.theme1-bt { background-color: #e0dfdf; }
      #navigator div.theme1-sp { background-color: #e0cfcf; }
      #navigator div.theme2 { background-color: #ffe0ff; }
      #navigator div.theme3 { background-color: #ffffe0; }
      
    </style>
  </head>

  <body onload="init()">
    
    <div id="bgpage" class="background"></div>
    
    <div class="header">
    </div>

    <div id="footer" class="footer">
      <div id="navigator" class="large"></div>
    </div>

    <div id="slides" class="slides">
      <div class="slide title"
           onshow="d0('navigator','footer');v0('bgpage')"
           onhide="d1('navigator','footer');v1('bgpage')">

        <h1>Linear Algebra for Machine Learning</h1>
        <h2>Tai-Danae, Mesch</h2>
        
        <div class="bottom">
          <h2 class="venue">X/Google, 2021</h2>
        </div>
      </div>
      
      <div class="slide build-focus-visible" group="intro" name="intro">
        <h1>Intro: Tensors!</h1>
        <div>The ML library we all use is called
          Tensorflow, after those things from Linear Algebra. But
          what <em>are</em> Tensors really?</div>
        <div>Important, is what they are!</div>
        <ul>
          <li step="1">In <b>General Relativity</b>, tensors describe
            Gravitation.
            
          <li step="2">In <b>Quantum Mechanics</b>, multi-particle states are
            the tensor products of single particle states.

          <li step="3">In <b>Quantum Computing</b>, each qubit is one factor of
            a tensor product.

          <li step="4">In <b>Machine Learning</b> ... well, those "tensors" in
            Tensorflow are just superficially connected with what they are in
            Mathematics -- we'll explain how exactly! -- but tensor products
            appear in the interesting places when optimizing weight matrices in
            large models.
        </ul>

        <div class="notes">Remember to look at the notes.</div>
      </div>

      <div class="slide build-focus-visible" group="intro" name="plan">
        <h1>Game Plan</h1>
        <div>Throughout the next 3 sessions:</div>

        <ul>
          <li step="1" link="theme1">We explain what tensors are, and how they
            come to be associated with multidimensional array data structures in
            software engineering, especially in machine learning.
            
          <li step="2" link="theme2">We contemplate how tensor products of
            vector spaces have a peculiar scaling behavior in their dimension:
            Unlike the Cartesian Product, which adds the dimensions of its
            factors, the Tensor Product multiplies. We also contemplate how
            confusing it is that $2+2=2*2=2^2$.

          <li step="3" link="theme3">We finally look how tensors and their
            products appear in machine learning models.
        </ul>
      </div>

      <div class="slide build-focus-visible" group="intro">
        <h1>Meta</h1>

        <div>Besides the improved understanding of ML and QC, there are a few
          tangential motivations that are inspiring in their own right, or for
          building software:</div>

        <ul>
          <li step="1">Introduce axiomatic construction.

          <li step="2">Look at Notation.

          <li step="3">Learn the same thing againi from a different
            perspective. (Remember the Tensorflow code retreat.)

          <li step="4">Introduce some mathematical concepts, such as
            Isomorphism and Duality.
        </ul>
      </div>

      <div class="slide noprint" group="theme1" name="theme1">
        <div class="banner">
          <div class="content">
            <b>I - Vectors and Tensors</b><br><em>Why, in software, arrays with multiple
            integer indices came to be called tensors.</em>
          </div>
        </div>
      </div>
      
      <div class="slide build-visible" group="theme1">
        <h1>Vectors and Tensors - Overview</h1>
        <div>Plan for today.</div>
        <ol>
          <li step="0">Recap Axioms of Vector Space

          <li step="1">Einstein Notation

          <li step="2">Linear Independence <em>gives rise to Basis and Coordinates</em>

          <li step="3">Basis and Coordinate Transformations

          <li step="4">The Scalar Product <span step="5">$\leftarrow$ This is
              the first Tensor!</span>

          <li step="6">Tensor Coordinates and Basis Transformations
        </ol>
      </div>

      <div class="slide build-focus-visible" group="theme1-vs">
        <h1>Axioms of Vector Space</h1>
        <div>Recap what Vectors are, and Vector Spaces.</div>

        <ul>
          <li step="0"><b>Vectors</b> can be <b>added</b> to each other and
            <b>multiplied with scalar</b> numbers.
            
          <li step="1"><em>Scalar numbers</em> are elements of a <b>Field</b>,
            usually <b>Rational Numbers</b>, <b>Real Numbers</b>,
            or <b>Complex Numbers</b>, but also <em>finite fields</em>,
            notably the <b>Galois Field</b>.
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-vs">
        <h1>Axioms of Vector Space</h1>
        <div>Everything that follows these axioms, is a vector space:</div>

        <ol>
          <li step="2">Associative: $(\vec{a} + \vec{b}) + \vec{c} = \vec{a} + (\vec{b} + \vec{c})$</li>
          <li step="3">Commutative: $\vec{a} + \vec{b} = \vec{b} + \vec{a}$</li>
          <li step="4">Neutral Element: $\exists \vec{0} \; \forall \vec{a} : \vec{a} + \vec{0} = \vec{a}$</li>
          <li step="5">Inverse Element: $\forall \vec{a} \; \exists -\!\vec{a} : \vec{a} + (-\vec{a}) = \vec{0}$</li>
          <li step="6">Scalar mult compatible with Field mult: $a \cdot (b \cdot \vec{a}) = (a \cdot b) \cdot \vec{a}$</li>
          <li step="7">Neutral Element of scalar mult: $1 \cdot \vec{a} = \vec{a}$</li>
          <li step="8">Distributive over vector add: $a \cdot \vec{a} + a \cdot \vec{b} = a \cdot (\vec{a} + \vec{b})$</li>
          <li step="9">Distributive over scalar add: $a \cdot \vec{a} + b \cdot \vec{a} = (a + b) \cdot \vec{a}$</li>
        </ol>

        <div class="notes">
          <ul>
            <li>$\forall$ is implied where absent in the axioms.
          </ul>
        </div>
      </div>

      <div class="slide build-focus-visible" group="theme1-vs">
        <h1>Axioms of Vector Space - Examples</h1>
        <div>Two examples of what vector spaces are.</div>
        <ul>
          <li step="1">Example 1: Arrows over Rational Numbers.
            
          <li step="2">Example 2: Tuples of Real Numbers over Real Numbers.
        </ul>
      </div>

      <div class="slide build-focus" group="theme1-vs">
        <h1>Axioms of Vector Space - Example: Arrows</h1>
        <div>Arrows in the 2D plane are vectors, with their operations defined
          by Euclidean Geometry.</div>
        <div class="img">
          <img class="img" src="img/vectors/vector-plus.png" step="1">
          <img class="img" src="img/vectors/vector-plus-result.png" step="2">
          <img class="img" src="img/vectors/vector-neg.png" step="3">
          <img class="img" src="img/vectors/vector-neg-result.png" step="4">
          <img class="img" src="img/vectors/vector-mult.png" step="5">
          <img class="img" src="img/vectors/vector-mult-result.png" step="6">
          <img class="img" src="img/vectors/vector-frac.png" step="7">
          <img class="img" src="img/vectors/vector-frac-result.png" step="8">
        </div>
      </div>
      
      <div class="slide build-focus-visible" group="theme1-vs">
        <h1>Axioms of Vector Space</h1>
        <div><b>Exercise:</b> Are these two examples vector spaces?</div>

        <ul>
          <li>(a) Tuples of Real Numbers over Rational Numbers?

          <li>(b) Tuples of Rational Numbers over Real Numbers?
        </ul>
      </div>
      
      <div class="slide build-focus-visible" group="theme1-li">
        <h1>Linear Independence</h1>
        <div step="0">The existence of <b>coordinates</b> follows directly from the
        axioms.</div>
        <div class="notes">
          <ul>
            <li>This derivation leads to the introduction of coordinates.

            <li>It follows directly form the algebraic structure introduced by
              the axioms.
          </ul>
        </div>

        <ul>
          <li step="1">Vectors can be <b>linearly combined</b>:

            $$\sum_{i} w_i \cdot \vec{v}_i$$

          <li step="2">Can such combinations yield $\vec{0}$ with coefficients
            that are not all $0$? (It's always possible with all coefficients 0,
            of course.)
            
          <li step="3">Vectors in the combination are then said to
            be <b>linearly dependent</b>.

          <li step="3">Otherwise they are <b>linearly indedendent</b>.

          <li step="4"><b>Exercise:</b> Imagine sets of arrows that are
            <ul>
              <li>(a) linearly dependent,
              <li>(b) linearly independent.
            </ul>
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-li">
        <h1>Einstein Notation</h1>
        <div>We introduce a notation for vector algebra that's very neat.</div>
        <ul>
          <li>Convention:

            $$w^i \vec{v}_i := \sum_{i} w_i \cdot \vec{v}_i$$

          <li>If an index $i$ appears in a multiplication expression both as
            upper index $w^i$ and as lower index $\vec{v}_i$, then the sum over
            the range of the index is implied.

          <li>Also for multiple indices. (Will see this for tensors.)
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-li">
        <h1>Einstein Notation</h1>
        <ul>
          <li>No sum for an index on its own:
            
            $$\vec{v}_i$$
            
            is simply a tuple of vectors.
          
          <li>No sum for a repeated index on top or bottom:
            
            $$M_{ii}$$
            
            is the tuple of the diagonal elements of the matrix $M_{ij}$.
          </li>
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-li">
        <h1>Linear Independence</h1>
        <div class="notes">
          More analysis yields interesting properties of linear independence.
        </div>

        <ul>
          <li step="0">In each vector space, the maximum number of elements $\vec{e}_i$
            in a set of linearly independent vectors is <b>fixed</b>.

          <li step="1,3">For every other vector $\vec{v}$ added to such a set, there is a
            linear combination that yields $\vec{0}$:

            $$v^{i}\vec{e}_{i} + v^{0}\vec{v} = \vec{0}$$</li>

          <li step="2,3">Another way of saying this is that $\vec{v}$ can be combined from
            $\vec{e}_{i}$. With the same coefficients as above, and noticing that
            $v^{0}$ cannot be $0$:
            
            $$\vec{v} = - \frac{v^{i}}{v^0} \vec{e}_{i}$$
            
            (Einstein notation applies.)

          <li step="3"><b>Exercise:</b> (a) why must be $v^{0} \neq 0$? (b) Why
            would it be a problem otherwise?</li>
        </ul>
      </div>


      <div class="slide build-visible" group="theme1-li">
        <h1>Linear Independence</h1>
        <div class="notes">
          One more property, and it all yields 3 important concepts.
        </div>

        <ul>
          <li step="0">Write more simply;
            
            $$\vec{v} = v^{i} \vec{e}_{i}$$

          <li step="1">Are these coefficients unique?

          <li step="2">It turns out yes they are!

          <li step="3">Proof follows, to show how such things work ...
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-li">
        <h1>Linear Independence</h1>
        <div step="0"><b>Proof</b> that the coefficients of any vector are unambiguous.</div>

        <ul>
          <li step="1">Assume there are two such sets of coefficients:

            $$\vec{v} = v^{i} \vec{e}_{i} = u^{i} \vec{e}_{i}$$

          <li step="2">subtract one from the other:

            $$\vec{v} - \vec{v} = v^{i} \vec{e}_{i} - u^{i} \vec{e}_{i}$$

            $$\vec{0} = (v^{i} - u^{i}) \cdot \vec{e}_{i}$$

          <li step="3">Now remember that $\vec{e}_i$ are linearly independent and that
            means $\vec{0}$ can be combined <em>only</em> with <b>all</b>
            coefficients $0$.
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-li">
        <h1>Linear Independence</h1>

        <ul>
          <li step="0">Thus for all $i$:

            $$v^{i} - u^{i} = 0$$

            (No Einstein sum here, because the index is up both times.)

          <li step="1">Or:

            $$v^{i} = u^{i}$$

            QED.
            
          <li step="2">Thus, every vector in the vector space can be represented as a
            combination with unambiguous coefficients from any maximal set of
            lineary independent vectors.</li>
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-bt">
        <h1>Dimension, Basis, and Coordinates</h1>
        <div step="0">From the concept of Linear Indepdendece, we arrive
          at <b>3 Definitions</b>:</div>

        <ul>
          <li step="1">The number of elements in a maximal set of linearly
            independent vectors of a vector space is called <b>Dimension</b> of
            that vector space.
            
          <li step="2">Any such set itself is called a <b>Basis</b> of that
            vector space.
            
          <li step="3">The coefficients in the linear combination of the basis
            vectors that yields a vector are called the <b>Coordinates</b> of
            the vector in that basis.
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-bt">
        <h1>Coordinate Tuples vs $F^n$ Vectors</h1>
        <ul>      
          <li step="0">Once a basis is picked, every vector is represented by
            a tuple of coordinates.

          <li step="1">The tuples are elements of $F^n$, the cartesian product space of
            the scalar field.

          <li step="2">The field $F$ by its own axioms has addition and multiplication
            defined. The cartesian product naturally has addition and scalar
            multiplication &mdash; $F^n$ is a vector space too!

          <li step="3">This vector space is <b>isomorphic</b> to the original
            vector space, which often misleads us to think that
            vectors just <em>are</em> tuples of numbers.

          <li step="4">The mapping of the vectors to its coordinate tuples is called
            an <b>Isomorphism</b>. There are many such mappings, one for each
            basis of the vector space.

          <li step="5"><b>Exercise:</b> Define two different bases in $R^2$, and
            compute the coordinates of one vector in both bases.
        </ul>
      </div>
      
      <div class="slide build-focus-visible" group="theme1-bt">
        <h1>Basis Transformation</h1>
        <div step="0">Let's consider <b>two</b> different Bases $\{\vec{e}_{i}\}$ and
          $\{\vec{e}_{i^\prime}\}$.</div>
        <ul>
          <li step="1"><b>Notation:</b> Symbols with differently primed index
            symbols refer to different objects. Thus $\vec{e}_{i}$ is a
            different vector from $\vec{e}_{i^\prime}$ even for equal values of
            $i$ and $i^\prime$.
            
          <li step="2">All vectors of the vector space have coordinates with
            regard to the first base as well as with regard to the second base:

            $$\vec{v} = v^{i} \vec{e}_{i} = v^{i^\prime} \vec{e}_{i^\prime}$$

          <li step="3,4">All vectors of the second basis have coordinates with
            regard to the first base. Let's call $T_{i^\prime}^{i}$ the
            coordinates of $\vec{e}_{i^\prime}$ in the basis ${\vec{i}_{i}}$:

            $$\vec{e}_{i^\prime} = T_{i^\prime}^{i} \vec{e}_{i}$$

          <li step="4"><b>Notation:</b> We could write $T$ as a matrix. But we
            don't, and stick to Einstein notation instead. We'll see later why.
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-bt">
        <h1>Basis Transformation</h1>
        <ul>
          <li step="0,1">All vectors of the first basis $\{\vec{e}_{i}\}$ have
            coordinates with regard to the second basis and
            $\{\vec{e}_{i^\prime}\}$ just like every vector:

            $$\vec{e}_{i} = T_{i}^{i^\prime} \vec{e}_{i^\prime}$$

          <li step="1"><b>Notation:</b> the indices on $T_{i}^{i^\prime}$ and
            $T_{i^\prime}^{i}$ are different, so the note from the previous
            slide applies.
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-bt">
        <h1>Basis Transformation</h1>
        <ul>
          <li step="0">So the two $T$ are different, but they are related, as we
            see from inserting one in the other:

            $$\vec{e}_{i} = T_{i}^{i^\prime} \vec{e}_{i^\prime}$$

            $$\vec{e}_{i} = T_{i}^{i^\prime} T_{i^\prime}^{j} \vec{e}_{j}$$

          <li step="1,2,3">thus:

            $$T_{i}^{i^\prime} T_{i^\prime}^{j} = \delta_{i}^{j}

            \;\;\mathrm{where}\;\;

            \delta_{i}^{j} = 1 \; (i=j), \; 0 \; (i \neq j)$$

          <li step="2">The two matrices $T_{i}^{i^\prime}$ and
            $T_{i^\prime}^{j}$ are each other's inverse.

          <li step="3"><b>Notation:</b> we don't have to care about transpose or
            ordering of factors.
        </ul>
      </div>
      
      <div class="slide build-focus-visible" group="theme1-bt">
        <h1>Basis Transformation</h1>
        <ul>
          <li step="0">We can ask for the relationship of the coordinates of a
            vector too:
            
            $$\vec{v} = v^{i} \vec{e}_{i} = v^{i^\prime} \vec{e}_{i^\prime}$$ 

            $$\vec{v} = v^{i} \vec{e}_{i} = v^{i^\prime} T_{i^\prime}^{i} \vec{e}_{i}$$ 

          <li step="1,2">thus:

            $$v^{i} = T_{i^\prime}^{i} v^{i^\prime}

            \;\; \mathrm{and\;remember} \;\;\ \vec{e}_{i} = T_{i}^{i^\prime} \vec{e}_{i^\prime}$$

          <li step="2">... basis vector and coordinates transform inverse (and
            transposed) to each other.
          
          <li step="3"><b>Notation:</b> again ordering of factors is not important for
            meaning, unlike in matrix notation.
        </ul>
      </div>
          
      <div class="slide build-focus-visible" group="theme1-bt">
        <h1>Basis Transformation</h1>
        <ul>
          <li step="0"><b>Definition:</b> $T$ is called a <b>basis
            transformation</b>, and correspondingly a <b>coordinate
            transformation</b>.

          <li step="1"><b>Exercise:</b> Write down the basis transformation and
            the coordinate transformation for a rotation of 90 degrees clockwise
            in space of arrows in the 2D plane.
        </ul>
      </div>
      
      <div class="slide build-focus-visible" group="theme1-sp">
        <h1>Scalar Product and Metric Tensor</h1>
        <div>Let's look at the scalar product in Vector Spaces.</div>
        <ul>
          <li step="0">The scalar product is a map from the vector space to its
            scalar field:

            $$\vec{v} \cdot \vec{w} = a \in F$$

          <li step="1">linear:

            $$(a \vec{v}) \cdot \vec{w} = a (\vec{v} \cdot \vec{w})$$
            
            $$(\vec{v} + \vec{u}) \cdot \vec{w} = \vec{v} \cdot \vec{w} + \vec{u} \cdot \vec{w}$$
            
          <li step="2">commutative:
            $\vec{v} \cdot \vec{w} = (\vec{w} \cdot \vec{v})$
            
          <li step="3">regular:
            $\forall \vec{v} \neq \vec{0} \; \exists \vec{w}: \vec{v} \cdot
            \vec{w} \neq 0$
        </ul>
      </div>
            
      <div class="slide build-focus-visible" group="theme1-sp">
        <h1>Scalar Product and Metric Tensor</h1>
        <div step="0">We are on the way to define the first tensor.</div>
        <ul>
          <li step="1">The vectors can be written with coordinates in a basis:

            $$\vec{v} \cdot \vec{w} = (v^{i}\vec{e}_{i}) \cdot (w^{j} \vec{e}_{j})$$

          <li step="2">... using the linearity of the scalar product yields:

            $$\vec{v} \cdot \vec{w} = v^{i} w^{j} (\vec{e}_{i} \cdot \vec{e}_{j})$$

          <li step="3">The scalar products of the basis vectors are just
            numbers. We call them $g$:

            $$g_{ij} := \vec{e}_{i} \cdot \vec{e}_{j}$$

          <li step="4">... then:

            $$\vec{v} \cdot \vec{w} = v^{i} w^{j} g_{ij}$$
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-sp">
        <h1>Scalar Product and Metric Tensor</h1>
        <ul>
          <li step="0">For a given basis, these numbers $g_{ij}$ fully define the scalar
            product in the vector space; they are the <em>coordinates</em> of
            the bilinear map that is the scalar product.


          <li step="1">We call them coordinates because they work like coordinates of
            vectors: The object they describe is not those numbers, it's a map
            of vectors to scalar numbers, just like the vectors are not their
            coordinates. But once vectors are described by coordinates relative
            to a basis, the map is described by coordinates too

          <li step="2"><b>Exercise:</b> Write down the coordinates of the scalar
            product for arrows in 2D space in a basis of your choice.
        </ul>
      </div>
      
      <div class="slide" group="theme1-sp">
        <h1>Scalar Product and Metric Tensor</h1>
        <div>Two interesting questions arise, both of which lead to the definition of tensors:</div>
        <ul>
          <li>(1) What are the coordinates of the scalar product in another
          basis?

          <li>(2) What are the "basis vectors" of whom the coordinates are the
            coefficients?
        </ul>

        <div><b>Exercise:</b> Discuss the questions above.</div>
      </div>

      <div class="slide build-focus-visible" group="theme1-sp">
        <h1>Scalar Product and Metric Tensor</h1>
        <div step="0">What are the coordinates of the scalar product in another
          basis? Lets see ...</div>
        <ul>
          <li step="1">$\vec{v} \cdot \vec{w} = (v^{i}\vec{e}_{i}) \cdot (w^{j} \vec{e}_{j})$
            
          <li step="2">$\vec{v} \cdot \vec{w} = (v^{i^\prime}\vec{e}_{i^\prime}) \cdot (w^{j^\prime} \vec{e}_{j^\prime})$

          <li step="3">$\vec{v} \cdot \vec{w} = (v^{i^\prime} T_{i^\prime}^{i} \vec{e}_{i}) \cdot (w^{j^\prime} T_{j^\prime}^{j} \vec{e}_{j})$

          <li step="4">$\vec{v} \cdot \vec{w} = v^{i^\prime} w^{j^\prime} \;
            T_{i^\prime}^{i} T_{j^\prime}^{j} \; (\vec{e}_{i} \cdot \vec{e}_{j})$

          <li step="5">$\vec{v} \cdot \vec{w} = v^{i^\prime} w^{j^\prime} \;
            T_{i^\prime}^{i} T_{j^\prime}^{j} \; g_{ij}$

          <li step="6">$\vec{v} \cdot \vec{w} = v^{i^\prime} w^{j^\prime} \;
            g_{i^{\prime}j^{\prime}}$

          <li step="7">thus, $g_{i^{\prime}j^{\prime}} = T_{i^\prime}^{i} T_{j^\prime}^{j} \; g_{ij}$

        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-sp">
        <h1>Tensors</h1>
        <div step="0">We can now give a definition of a Tensor, and of the Tensor
          Product Space.</div>
        <ul>
          <li step="1">A <b>Tensor</b> is any mathematical object in a vector space with
            coordinates that transform under basis transformations like a
            multilinear map.

          <li step="2">The scalar product is just one such map. There are many others.

          <li step="3">They can even be combined, e.g.:
            
            $$(g + h)(\vec{a}, \vec{b}) := g(\vec{a}, \vec{b}) + h(\vec{a},
            \vec{b})$$
            
            $$(ag)(\vec{a}, \vec{b}) := a \cdot g(\vec{a}, \vec{b})$$

          <li step="4">Looking closely, they form a vector space too!

          <li step="5">The <b>Tensor Product Space</b> of $V$ is the vector space
            comprised of tensors in $V$:

            $$g,h \in V \otimes V$$
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-sp">
        <h1>Scalar Product and Metric Tensor</h1>
        <div step="0">Now that we have defined Tensors, a few more Definitions.</div>
        <ul>
          <li step="1">A vector space with a scalar product is a <b>Euclidean Vector
              Space</b>.
            
          <li step="2">Two vectors from a Euclidean Vector Space whose scalar product is
            $0$ are <b>Orthogonal</b>.

          <li step="3">If the scalar product is positive definite (not always the case),
            then he scalar product also gives rise to a <b>Norm</b>:

            $$|\vec{x}| = \sqrt{\vec{x} \cdot \vec{x}}$$

          <li step="4">The <b>Angle</b> $\phi$ between two vectors $\vec{x}$ and
            $\vec{y}$ is defined by

            $$\cos \phi = \frac{\vec{x} \cdot \vec{y}}{\sqrt{\vec{x} \cdot
            \vec{x}} \sqrt{\vec{y} \cdot \vec{y}}}$$
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-sp">
        <h1>Scalar Product and Metric Tensor</h1>
        <ul>
          <li step="0">Because the scalar product defines both distance and
            direction in a Euclidean Vector Space, i.e. the <em>Metric</em>, and
            is a tensor, it's called the <b>Metric Tensor</b>.

          <li step="1">In <b>Riemann Geometry</b>, the metric tensor is in every
            point in space (i.e. it's a tensor field). In <b>General
            Relativity</b>, the metric tensor is connected to the mass by a
            partial differential equation.
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-sp">
        <h1>Higher Order Tensors</h1>
        <ul>
          <li step="0"><b>Tensor Product Spaces</b> of $V$ arise from multilinear maps in
            the same way, e.g.:
            
            $$V \otimes V \otimes V \otimes V$$

          <li step="1">Such tensors have coordinates with as many indices:

            $$g_{ijkl}$$

          <li step="2">that transform under a basis transformation:

            $$g_{i^{\prime}j^{\prime}k^{\prime}l^{\prime}} =
            T_{i^\prime}^{i}
            T_{j^\prime}^{j}
            T_{k^\prime}^{k}
            T_{l^\prime}^{l}
            g_{ijkl}$$

          <li step="3">Natural question: What's the dimension of that space, and what
            is its basis?
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-sp">
        <h1>Tensor Bases</h1>
        <div step="0">We answer the second of those questions first, and the
          first in the next session ...</div>
        <ul>
          <li step="1">Given a basis in $V$, we define a basis in $V \otimes V$ as follows:

            $${\vec{e}^i \otimes \vec{e}^j}$$

          <li step="2">is the bilinear function of two vectors from $V$ such that

            $$(\vec{e}^i \otimes \vec{e}^j)(\vec{e}_k, \vec{e}_l) = \delta^i_k
            \delta^j_l$$

          <li step="3">then a bilinear map $g$ with coordinates $g_{ij}$ is given by

            $$g = g_{ij} \, \vec{e}^i \otimes \vec{e}^j$$
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1-sp">
        <h1>Tensor Bases</h1>
        <ul>
          <li step="0">
            $g(\vec{x}, \vec{y}) = g_{ij} (\vec{e}^i \otimes \vec{e}^j)(x^k
            \vec{e}_k, y^l \vec{e}_l)$

          <li step="1">
            $g(\vec{x}, \vec{y}) = g_{ij} x^k y^l (\vec{e}^i \otimes \vec{e}^j)(\vec{e}_k, \vec{e}_l)$

          <li step="2">
            $g(\vec{x}, \vec{y}) = g_{ij} x^k y^l \delta^i_k \delta^j_l$

          <li step="3">
            $g(\vec{x}, \vec{y}) = g_{ij} x^i y^j$

          <li step="4">
            as we said earlier.
            
        </ul>
      </div>

      <div class="slide build-focus-visible" group="theme1">
        <h1>Conclusion</h1>
        <ul>
          <li step="0">Vectors can be added and multiplied.
          <li step="1">From that <b>follows</b> they have coordinates wrt a basis.
          <li step="2">Tensors are multilinear functions of vectors.
          <li step="3">They are themselves vectors (can be added and
            multiplied), and thus have coordinates wrt a basis.
          <li step="4">The basis and coordinates of the tensors are related to
            the basis of and coordinates of the vectors they are functions of.
        </ul>
      </div>
      
      <div class="slide" group="theme1">
        <h1>Next</h1>
        <ul>
          <li>We look at the dimension of tensor product spaces, and how tensor
            products appear in Quantum Computing, but also in data structures in
            Software Engineering.
        </ul>
      </div>

      <div class="slide"></div>
    </div>
  </body>
</html>
