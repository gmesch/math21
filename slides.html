<!doctype html>
<html>
  <head>
    <title>Math for ML</title>
    
    <script src="lib/jslib.js"></script>
    
    <script src="lib/slides.js"></script>
    <link href="lib/slides.css" rel="stylesheet">
    
    <script src="lib/state.js"></script>
    <script src="lib/state_util.js"></script>
    
    <script src="mathjax/tex-mml-chtml.js" async></script>
    <script src="prettify/run_prettify.js?autorun=false&lang=scm" async></script>

    <link href="fonts/fonts.css" rel="stylesheet">

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
        }
      };
    </script>
    
    <script>
      function init() {
        initslides();

        preAdjust();
        PR.prettyPrint();

        console.log('slides init done');
      }
    </script>

    <style>
      div.img { position: relative }
      img.img { position: absolute }
    </style>
  </head>

  <body onload="init()">
    
    <div id="bgpage" class="background"></div>
    
    <div class="header">
      <div id="navigator" class="large"></div>
    </div>

    <div id="footer" class="footer">
    </div>

    <div id="slides" class="slides">
      <div class="slide title"
           onshow="d0('navigator','footer');v0('bgpage')"
           onhide="d1('navigator','footer');v1('bgpage')">

        <h1>Linear Algebra for Machine Learning</h1>
        <h2>Tai-Danae, Mesch</h2>
        
        <div class="bottom">
          <h2 class="venue">X/Google, 2021</h2>
        </div>
      </div>
      
      <div class="slide build-focus" name="intro">
        <h1>Intro: Tensors!</h1>
        <p>The ML library we all use is called Tensorflow, after those things
          from Linear Algebra. But what are tensors really?</p>

        <p>Important, is what they are!
          <ul>
            <li step="1">In <b>General Relativity</b>, tensors describe Gravitation</li>
            <li step="2">In <b>Quantum Mechanics</b>, multi-particle states are the tensor
              products of single particle states</li>
            <li step="3">In <b>Quantum Computing</b>, every qubit is one factor of a tensor
              product.</li>
            <li step="4">In <b>Machine Learning</b> ... well, those "tensors" in Tensorflow are
              just superficially connected with what they are in Mathematics
              -- we'll explain how exactly! -- but tensor products appear in the
              interesting places when optimizing weight matrices in large
              models.</li>
          </ul>
        </p>

        <div class="notes">Remember to look at the notes.</div>
      </div>

      <div class="slide build-focus" name="plan">
        <h1>Game Plan</h1>
        Throughout the next 3 sessions:
        <ol>
          <li step="1" link="theme1">We explain what tensors are, and how they
            come to be associated with multidimensional array data structures in
            software engineering, especially in machine learning</li>
          <li step="2" link="theme2">We contemplate how tensor products of
            vector spaces have a peculiar scaling behavior in their dimension:
            Unlike the Cartesian Product, which adds the dimensions of its
            factors, the Tensor Product multiplies. We also contemplate how
            confusing it is that \(2+2=2*2=2^2\)</li>
          <li step="3" link="theme3">We finally look how tensors and their
            products appear in machine learning models</li>
        </ol>
      </div>

      <div class="slide fit" name="fit">
        Fit me already.
      </div>
      
      <div class="slide noprint" group="intermission" name="theme1">
        <div class="banner">
          <div class="content">
            <b>I - Vectors and Tensors</b><br><em>Why, in software, arrays with multiple
            integer indices came to be called tensors.</em>
          </div>
        </div>
      </div>
      
      <div class="slide build">
        <h1>Vectors and Tensors - Overview</h1>
        <p>Axioms of Vector Space</p>
        <p>Einstein Notation</p>
        <p>Linear Dependence gives rise to Basis and Coordinates</p>
        <p>Basis and Coordinate Transformations</p>
        <p>The Scalar Product <span step="1">This is the first
            Tensor!</span></p>
        <p>Tensor coordinates and transformations</p>
      </div>

      <div class="slide build-focus-visible">
        <h1>Axioms of Vector Space</h1>
        <div style="float:right;width:50%">Vectors are things that can be added
          to each other and multiplied with scalar numbers.  $V$ is a tuple $(V,
          +, F, *)$:vectors, scalars, addition, scalar multiplication</div>
        <ol>
          <li step="1">$(\vec{a} + \vec{b}) + \vec{c} = \vec{a} + (\vec{b} + \vec{c})$</li>
          <li step="2">$\vec{a} + \vec{b} = \vec{b} + \vec{a}$</li>
          <li step="3">$\exists \vec{0} \in V \forall \vec{a} \in V : \vec{a} + \vec{0} = \vec{a}$</li>
          <li step="4">$\forall \vec{a} \in V \exists -\vec{a} \in V : \vec{a} + (-\vec{a}) = \vec{0}$</li>
          <li step="5">$a * (b * \vec{a}) = (a * b) * \vec{a}$</li>
          <li step="6">$1 * \vec{a} = \vec{a}$</li>
          <li step="7">$a * \vec{a} + a * \vec{b} = a * (\vec{a} + \vec{b})$</li>
          <li step="8">$a * \vec{a} + b * \vec{a} = (a + b) * \vec{a}$</li>
        </ol>
        <p>For example, arrows.</p>
        <p>Another example, tuples of numbers from $R^n$.</p>
      </div>

      <div class="slide build-focus">
        <h1>For example, Arrows</h1>
        <div class="img">
          <img class="img" src="img/vectors/vector-plus.png" step="1">
          <img class="img" src="img/vectors/vector-neg.png" step="2">
          <img class="img" src="img/vectors/vector-mult.png" step="3">
          <img class="img" src="img/vectors/vector-frac.png" step="4">
        </div>
      </div>

      <div class="slide">
        <h1>Linear Independence</h1>
        <div>This derivation leads to the introduction of coordinates. Notice it
          follows directly form the algebraic structure introduced by the
          axioms.</div>
        <ul>
          <li>Vectors can be linearly combined $\sum_{i} w_i * \vec{v}_i$</li>
          <li>Can such combinations yield $\vec{0}$ with coefficients that are
            not all $0$? (It's always possible with all coefficients 0, of
            course.)</li>
          <li>Vectors in the combination are then said to be <b>linearly dependent</b>.</li>
          <li>Otherwise they are <b>linearly indedendent</b>.</li>
          <li>Exercise: Try to imagine sets of arrows that are (a) linearly
            dependent, (b) linearly independent.</li>
        </ul>
      </div>

      <div class="slide">
        <h1>Aside: Einstein Notation</h1>
        <div>This convention is incredibly useful:

          $$\sum_{i} w_i * \vec{v}_i$$
          
          is also written as

          $$w^i \vec{v}_i$$

          The convention is that if an index, here $i$, appears in an expression
          both as upper index, here $w^i$, and as lower index, here $\vec{v}_i$,
          then a sum over an implied range of the index is implied.</div>

        <ul>
          <li>An index on its own just indicates a tuple with one element for
            each value of the index:
            
            $$\vec{v}_i$$
            
            is a tuple of vectors.</li>
          
          <li>A repeated index on top or bottom is also just such a tuple,
            with no sum implied:
            
            $$M_{ii}$$
            
              is the tuple formed from the diagonal elements of the matrix
            $M_{ij}$.
          </li>
      </div>

      <div class="slide">
        <h1>Linear Independence, continued</h1>
        <div>Analysis yields interesting properties of linear independence.</div>
        <ul>
          <li>In each vector space, the maximum number of elements $\vec{e}_i$
            in a set of linearly independent vectors is <b>fixed</b>.</li>
          <li>For every other vector $\vec{v}$ added to such a set, there is a
            linear combination that yields $\vec{0}$: $$v^{i}\vec{e}_{i} + v^{0}.</li>
          <li>Another way of saying this is that $\vec{v}$ can be combined from
            $\e_{i}$. With the same coefficient as above, and noticing that
            $v^{0}$ cannot be $0$:
            
            $$\vec{v} = - \frac{v^{i}}{v^0} \vec{e}_{i}$$

            (Einstein convention applies.)
          <li>Quick exercise: (a) why must be $v^{0} \neq 0$? (b) Why would it
            be problem otherwise?</li>
        </ul>
      </div>


      <div class="slide">
        <h1>Linear Independence, continued</h1>
        <div>One more property, and it all yields 3 important concepts.</div>
        <ul>
          <li>We simplify the coefficients:
            
            $$\vec{v} = v^{i} \vec{e}_{i}$$

          </li>
          <li>Are these coefficients unique? It turns out yes there are!</li>
          <li>Proof by assuming there are two different sets of coefficients,
            and concluding they must be equal:

            $$\vec{v} = v^{i} \vec{e}_{i} = u^{i} \vec{e}_{i}$$

            subtract one from the other:

            $$\vec{v} - \vec{v} = v^{i} \vec{e}_{i} - u^{i} \vec{e}_{i}$$

            $$\vec{0} = (v^{i} - u^{i}) * \vec{e}_{i}$$

            Now remember that $\vec{e}_i$ are linearly independent and that
            means $\vec{0}$ can be combined <em>only</em> with <b>all</b>
            coefficients $0$. Thus for all $i$:

            $$v^{i} - u^{i} = 0$$

            (Notice btw. no Einstein sum here, because the index is up both
            times.)

            $$v^{i} = u^{i}$$
          </li>
            
          <li>Thus, every vector in the vector space can be represented as a
            combination with unambiguous coefficients from any maximal set of
            lineary independent vectors.</li>

          <li>The same approach to proof that all maximal sets have same number
            of elements.</li>
        </ul>
      </div>

      <div class="slide">
        <h1>Linear Independence, end</h1>
        <div>So we arrive at 3 definitions:</div>
        <ol>
          <li>The number of elements in a maximum set of linearly independent
            vectors of a vector space is called <b>dimension</b> of that vector
            space.</li>
          <li>Any such set is called a <b>basis</b> of that vector space.</li>
          <li>The coefficients in the linear combination of the basis vectors
            that yields any vector are called the <b>coordinates</b> of the
            vector in that basis.</li>
          <li>Thus, every vector can unambiguously be represented by the tuple
            of its coordinates, once a basis is picked.</li>
          <li>The tuples are elements of $\F^n$, teh cartesian product space of
            the scalar field. Now, since the field $F$ already has addition and
            multiplication defined (it's a field after all), the cartesian
            product naturally has addition and scalar multiplication. Thus it's
            a vector space too!</li>
          <li>This vector space is <b>isomorphic</b> to the original vector
            space, which often misleads people to think that all vectors are
            tuples of numbers.</li>
          <li>The mapping of the vectors to its coordinate tuples is called
            an <b>Isomorphism</b>. There are many such mappings, one for each
            basis of the vector space.</li>
        </ol>
      </div>
      
      <div class="slide">
        <h1>Basis Transformation</h1>
        <div>Let's consider <b>two</b> different bases $\{\vec{e}_{i}\}$ and
          $\{\vec{e}_{j^\prime}\}$.</div>
        <ol>
          <li>Note on Notation: Symbols with differently primed index symbols
            refer to different objects. Thus $\vec{e}_{i}$ is a different vector
            from $\vec{e}_{j^\prime}$ even for equal values of $i$ and
            $j^\prime$.</li>
          <li>First observation: All vectors of the vector space have
            coordinates with regard to the
            first base as well as with regard to the second base:
            $$\vec{v} = v^{i} \vec{e}_{i} = v^{i^\prime} \vec{e}_{i^\prime}$$ 
          </li>
          <li>Second observation: All vectors of the second basis have
            coordinates with regard to the first base. Let's call these $T$:
            $$\vec{e}_{i^\prime} = T_{i^\prime}^{i} \vec{e}_{i}$$
            (We could write T as a matrix. But we don't, but keep it in
            Einstein notation instead. We'll see later why.)
          </li>
        </ol>
      </div>

      <div class="slide">
        <h1>Basis Transformation, continued</h1>
        <ol>
          <li>Third observation: All vectors of the first basis have coordinates
            with regard to the second basis too:
            $$\vec{e}_{i} = T_{i}^{i^\prime} \vec{e}_{i^\prime}$$
            Notice the indices on the two $T$s are different, so the note from
            the top applies.</li>
          <li>So the two $T$ objects are different, but they are related, as we
            see from inserting one in the other:
            $$\vec{e}_{i} = T_{i}^{i^\prime} \vec{e}_{i^\prime}$$
            $$\vec{e}_{i} = T_{i}^{i^\prime} T_{i^\prime}^{j} \vec{e}_{j}$$
            thus:
            $$T_{i}^{i^\prime} T_{i^\prime}^{j} = \delta_{i}^{j}$$
            where $\delta_{i}^{j}$ is $1$ if $i=j$ and $0$ if $i \neq j$</li>
          <li>The two matrices are each other's inverse.</li>
          <li>Notice in Einstein notiation we don't have to care about transpose
            or ordering of factors.</li>
        </ol>
      </div>
      
      <div class="slide">
        <h1>Basis Transformation, end</h1>
        <ol>
          <li>We can ask for the relationship of the coordinates of a vector
            too:
            $$\vec{v} = v^{i} \vec{e}_{i} = v^{i^\prime} \vec{e}_{i^\prime}$$ 
            $$\vec{v} = v^{i} \vec{e}_{i} = v^{i^\prime} T_{i^\prime}^{i} \vec{e}_{i}$$ 
            thus:
            $$v^{i} = v^{i^\prime} T_{i^\prime}^{i} = T_{i^\prime}^{i}
            v^{i^\prime}$$
            (Again ordering of factors is not important for meaning, unlike
            Matrix notation.)
          </li>
          <li>And remember:
            $$\vec{e}_{i} = T_{i}^{i^\prime} \vec{e}_{i^\prime}$$
            Basis vector and coordinates transform inverse (and transposed) to each other.
          </li>
          <li><b>Definition:</b> $T$ is called a <b>basis transformation</b>, and
            correspondingly a <b>coordinate transformation</b>.</li>
          <li>Exercise: Write down the basis transformation and the coordinate
            transformation for a rotation of 90 degrees clockwise in space of
            arrows in the 2D plane.</li>
        </ol>
      </div>
      
      <div class="slide">
        <h1>Scalar Product and Metric Tensor</h1>
        <div>We are on the way to define the first tensor.</div>
        <ol>
          <li>Let's look at the scalar product. It's a bilinear map from the
            vector space to its scalar field:
            $$\vec{v} \cdot \vec{w}$$
            the value of that is a scalar number and it holds that
            $$(a \vec{v}) \cdot \vec{w} = a (\vec{v} \cdot \vec{w})$$
            and
            $$\vec{v} \cdot (b \vec{w}) = b (\vec{v} \cdot \vec{w})$$
          </li>
          <li>Note we don't yet require postive definiteness.</li>
          <li>We can represent the vectors in the scalar product using coordinates
            in a basis:
            $$\vec{v} \cdot \vec{w} = (v^{i}\vec{e}_{i}) \cdot (w^{j} \vec{e}_{j})$$
            and apply the linearity of the scalar product:
            $$\vec{v} \cdot \vec{w} = v^{i} w^{j} (\vec{e}_{i} \cdot \vec{e}_{j})$$
            like the scalar products of every vector, the scalar products of the
            basis vectors are just numbers. We call them $g$:
            $$g_{ij} := \vec{e}_{i} \cdot \vec{e}_{j}$$
            then:
            $$\vec{v} \cdot \vec{w} = v^{i} w^{j} g_{ij}$$
            For a given basis, these numbers fully define the scalar product in
            the vector space; they are the coordinates of the bilinear map that
            is the scalar product.
          </li>
          <li>Exercise: Write down the coordinates of the scalar product for
            arrows in 2D space in a basis of your choice.</li>
        </ol>
      </div>
      
      <div class="slide">
        <h1>Scalar Product and Metric Tensor</h1>
        <div>Two interesting questions arise:</div>
        <ol>
          <li>What are the coordinates of
            the scalar product in another basis?</li>
          <li>What are the "basis
            vectors" of whom the coordinates are the coefficients?</li>
        </ol>
        <div>Both leads to the definition of tensors.</div>
        <div>Exercise: Discuss the questions above.</div>
      </div>

      <div class="slide">
        <h1>Scalar Product and Metric Tensor</h1>
        <div>What are the "basis
            vectors" of whom the coordinates are the coefficients?</div>
        <ol>
          <li>To answer this, we need one more conceptual ingredient.</li>
          <li>Linear functionals: A linear function from a vector space to its
            field.
            $$a = \mathbf{f}(\vec{x}) x \in V a \in \R$$
            linearity:
            $$\mathbf{f}(a \vec{x}) = a \mathbf{f}(\vec{x})$$
            $$\mathbf{f}(\vec{x} + \vec{y}) = \mathbf{f}(\vec{x}) + \mathbf{f}(\vec{y})$$
          </li>
          <li>From the coordinate representation of $\vec{x}$ this leads
            naturally to a coordinate representation of $\mathbf{f}$, like the
            one we
            saw for $g$:
            $$\mathbf{f}(\vec{x}) = \mathbf{f}(x^{i} \vec{e}_{i})$$
            $$\mathbf{f}(\vec{x}) = x^{i} \mathbf{f}(\vec{e}_{i})$$
            $$\mathbf{f}(\vec{x}) = x^{i} f_{i}$$
            where
            $$f_i := \mathbf{f}(\vec{e}_{i})$$
            (Einstein notation again.) We define $f_{i}$ to be teh coordinates
            of the functional. The question is, again, in what basis?
          </li>
        </ol>
      </div>
      <div class="slide">
        <h1>Scalar Product and Metric Tensor</h1>
        <ol>
          <li>There are many different such functions of course. Now, we define
            combination/construction rules for any of them:
            $$(a\mathbf{f})(\vec{x}) := a \mathbf{f}(\vec{x})$$
            $$(\mathbf{f} + \mathbf{h})(\vec{x}) := \mathbf{f}(\vec{x}) + \mathbf{h}(\vec{x})$$</li>
          <li>
            With these rules, the linear functionals of a vector space become a
            vector space themselves. It's called the <b>Co Vector Space</b> of
            the underlying vector space. Both share the same scalar field.</li>
          <li>Like for any vector space, the observations about linear
            independence apply, and lead to the existence of coordinates and
            bases.</li>
        </ol>
      </div>

      <div class="slide">
        <h1>Scalar Product and Metric Tensor</h1>
        <ol>      
          <li>We have seen another way to construct coordinates for
            covectors. How do we align the two? What is the basis that
            corresponds to the coordinates we defined above? Let's try:
            $$\mathbf{f} := f_{i} \mathbf{f}^{i}$$
          <li>we apply this to a vector:
            $$\mathbf{f}(\vec{v}) = (f_{i} \mathbf{f}^{i})(x^j \vec{x}_{j})$$
          <li>from linearity of the $\mathbf{f}^{i}$ follows:
            $$\mathbf{f}(\vec{v}) = f_{i} x^j \mathbf{f}^{i}(\vec{x}_{j})$$
          <li>Now, let's pick the basis $\mathbf{f}^{i}$ such that
            $$\mathbf{f}^{i}(\vec{x}_{j}) = \delta^i_j$$
          <li>If we pick these bases, then the application of a functional to a
            vector becomes the Einstein sum of the products of their
            coordinates: $$\mathbf{f}(\vec{v}) = f_{i} x^j \delta^i_j = f_{i}
            x^i$$
          </li>
        </ol>
      </div>
      
      <div class="slide">
        <h1>Scalar Product and Metric Tensor</h1>
        <div>Finally define the tensor product</div>
        <ol>      
          <li>We have seen another way to construct coordinates for
            covectors. How do we align the two? What is the basis that
            corresponds to the coordinates we defined above? Let's try:
            $$\mathbf{f} := f_{i} \mathbf{f}^{i}$$
          <li>we apply this to a vector:
            $$\mathbf{f}(\vec{v}) = (f_{i} \mathbf{f}^{i})(x^j \vec{x}_{j})$$
          <li>from linearity of the $\mathbf{f}^{i}$ follows:
            $$\mathbf{f}(\vec{v}) = f_{i} x^j \mathbf{f}^{i}(\vec{x}_{j})$$
          <li>Now, let's pick the basis $\mathbf{f}^{i}$ such that
            $$\mathbf{f}^{i}(\vec{x}_{j}) = \delta^i_j$$
          <li>If we pick these bases, then the application of a functional to a
            vector becomes the Einstein sum of the products of their
            coordinates: $$\mathbf{f}(\vec{v}) = f_{i} x^j \delta^i_j = f_{i}
            x^i$$
          </li>
        </ol>
      </div>
      
      <div class="slide">
        <h1>Duality and Isomorphism</h1>
        TANGENT
        <ol>
          <li>Like for any vector space, we can define linear functionals on the
            co vector space too.
            $$\xi(f) = a$$
            How can such a functional be constructed? For example, pick one
            vector $\vec{x}$ from the vector space, and define a functional:
            $$\xi(f) := f(\vec{x})$$
            It turns out <b>every</b> linear functional of the co vector space
            can be written in this way. There is a 1:1 correspondence between
            the linear functionals of the co vector space and the vectors of its
            vector space.</li>
          <li>Thus, we say that the co vector space of the co vector space
            <b>is</b> the vector space again.</li>
          <li>Btw. this introduces two important concepts: <b>Isomorphism</b>
            and <b>Duality</b>.</li>
          <li>Isomorphism is also how we got to think of vectors as <b>the
              same</b> as their coordinate tuples: Every vector space of
            dimension $n$ is &mdash; not the same as! but: &mdash; <b>isomorphic</b> to
              $\R^{n}$ with regard to the vector space operations addition and
            scalar multiplication.</li>
        </ol>
      </div>

      <div class="slide">
        <h1>Scalar Product and Metric Tensor</h1>
        <div>What are the coordinates of
            the scalar product in another basis?</div>
        <ol>
        </ol>
      </div>

      <div class="slide">
        <h1>Tensor Coordinate Transformations</h1>
        <p></p>
      </div>

      <div class="slide">
        <h1>Conclusion</h1>
        <p></p>
      </div>

      <div class="slide noprint" group="intermission" name="theme2">
        <div class="banner">
          <div class="content">
            II - The Dimension of Tensor Spaces (Dimensionality of the vector spaces of higher order tensors.)
          </div>
        </div>
      </div>
      
      <div class="slide print" group="content">
        <h1>Code Snippets</h1>
        <div class="snippet">
          <div class="label">state_base.js</div>
          <pre class="prettyprint lang-scm">
            (friends '(of) '(parentheses))
            function $(id) {
              return document.getElementById(id);
            }
          </pre>
          <pre class="prettyprint lang-js">
            (friends '(of) '(parentheses))
            function $(id) {
              return document.getElementById(id);
            }
          </pre>
        </div>
      </div>
      
      <div class="slide">
        <h1>Game Plan</h1>
        <p></p>
      </div>

      
      <div class="slide">
        <h1>Conclusion</h1>
        <p></p>
      </div>

      <div class="slide noprint" group="intermission" name="theme3">
        <div class="banner">
          <div class="content">
            III - Tensors in Tensorflow
          </div>
        </div>
      </div>

      <div class="slide">
        <h1>Conclusion</h1>
        <p></p>
      </div>
      
      <div class="slide"></div>
    </div>
  </body>
</html>
